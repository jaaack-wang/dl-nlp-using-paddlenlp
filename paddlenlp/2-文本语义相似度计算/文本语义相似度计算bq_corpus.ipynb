{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业\n",
    "\n",
    "完成预测环节预训练模型的调用代码，并跑通整个项目，成功提交千言文本相似度竞赛，按要求截图，提交作业即可。\n",
    "\n",
    "tips：\n",
    "\n",
    "- 预测可以使用自己训练的模型（训练时间较长），也可以直接使用提供下载的模型权重；\n",
    "- 报名千言文本相似度竞赛，并成功提交结果；\n",
    "- 并将如下图所示的结果截图，贴到本项目作业最后一行即完成作业。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cf119d3bc6504c098cc3cc58597b7061890d5fe915364f5fbd52341033307c7c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于预训练模型 ERNIE-Gram 实现语义匹配\n",
    "\n",
    "\n",
    "6.7NLP直播打卡课即将开播，欢迎大家关注课程，有任何问题来评论区或**QQ群**（群号:758287592）交流吧~~\n",
    "\n",
    "**[直播链接请戳这里，每晚20:00-21:30👈](http://live.bilibili.com/21689802)**\n",
    "\n",
    "**[课程地址请戳这里👈](https://aistudio.baidu.com/aistudio/course/introduce/24177)**\n",
    "\n",
    "\n",
    "本案例介绍 NLP 最基本的任务类型之一 —— 文本语义匹配，并且基于 PaddleNLP 使用百度开源的预训练模型 ERNIE-Gram 搭建效果优异的语义匹配模型，来判断 2 段文本语义是否相同。\n",
    "\n",
    "## 1. 背景介绍\n",
    "文本语义匹配任务，简单来说就是给定两段文本，让模型来判断两段文本是不是语义相似。\n",
    "\n",
    "在本案例中以权威的语义匹配数据集 [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) 为例，[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) 数据集是基于百度知道相似问题推荐构造的通问句语义匹配数据集。训练集中的每两段文本都会被标记为 1（语义相似） 或者 0（语义不相似）。更多数据集可访问[千言](https://www.luge.ai/)获取哦。\n",
    "\n",
    "例如百度知道场景下，用户搜索一个问题，模型会计算这个问题与候选问题是否语义相似，语义匹配模型会找出与问题语义相似的候选问题返回给用户，加快用户提问-获取答案的效率。例如，当某用户在搜索引擎中搜索 “深度学习的教材有哪些？”，模型就自动找到了一些语义相似的问题展现给用户:\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ecc1244685ec4476b869ce8a32d421c0ad530666e98d487da21fa4f61670544f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.快速实践\n",
    "\n",
    "介绍如何准备数据，基于 ERNIE-Gram 模型搭建匹配网络，然后快速进行语义匹配模型的训练、评估和预测。\n",
    "\n",
    "### 2.1 数据加载\n",
    "为了训练匹配模型，一般需要准备三个数据集：训练集 train.tsv、验证集dev.tsv、测试集test.tsv。此案例我们使用 PaddleNLP 内置的语义数据集 [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) 来进行训练、评估、预测。\n",
    "\n",
    "训练集: 用来训练模型参数的数据集，模型直接根据训练集来调整自身参数以获得更好的分类效果。\n",
    "\n",
    "验证集: 用于在训练过程中检验模型的状态，收敛情况。验证集通常用于调整超参数，根据几组模型验证集上的表现，决定采用哪组超参数。\n",
    "\n",
    "测试集: 用来计算模型的各项评估指标，验证模型泛化能力。\n",
    "\n",
    "[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) 数据集是公开的语义匹配权威数据集。PaddleNLP 已经内置该数据集，一键即可加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/e9/128dfc1371db3fc2fa883d8ef27ab6b21e3876e76750a43f58cf3c24e707/paddlenlp-2.0.2-py3-none-any.whl (426kB)\n",
      "\u001b[K     |████████████████████████████████| 430kB 598kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.1\n",
      "    Uninstalling paddlenlp-2.0.1:\n",
      "      Successfully uninstalled paddlenlp-2.0.1\n",
      "Successfully installed paddlenlp-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# 正式开始实验之前首先通过如下命令安装最新版本的 paddlenlp\n",
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.datasets import load_dataset, MapDataset\n",
    "import paddlenlp\n",
    "\n",
    "# 一键加载 Lcqmc 的训练集、验证集\n",
    "# train_ds, dev_ds = load_dataset(\"lcqmc\", splits=[\"train\", \"dev\"])\n",
    "\n",
    "\n",
    "# # 下载并解压数据集\n",
    "# from paddle.utils.download import get_path_from_url\n",
    "# URL = \"https://dataset-bj.cdn.bcebos.com/qianyan/paws-x-zh.zip\"\n",
    "# get_path_from_url(URL, \"./\")\n",
    "\n",
    "\n",
    "# bq_corpus can be downloaded from: https://aistudio.baidu.com/aistudio/datasetdetail/78992\n",
    "\n",
    "def load_dataset(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            next(fp)  # Skip header\n",
    "            for line in fp.readlines():\n",
    "                items = line.strip('\\n').split('\\t')\n",
    "                sens1 = items[0].split('\\002')[0]\n",
    "                sens2 = items[1].split('\\002')[0]\n",
    "                labels = items[-1].split('\\002')[0]\n",
    "                yield sens1, sens2, labels\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafile)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "# Create dataset, tokenizer and dataloader.\n",
    "train_ds, dev_ds, test_ds = load_dataset(datafiles=(\n",
    "        './bq_corpus/train.tsv', './bq_corpus/dev.tsv', './bq_corpus/test.tsv'))\n",
    "test_ds_copy = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('微信消费算吗', '还有多少钱没还', '0')\n",
      "('交易密码忘记了找回密码绑定的手机卡也掉了', '怎么最近安全老是要改密码呢好麻烦', '0')\n",
      "('你好我昨天晚上申请的没有打电话给我今天之内一定会打吗？', '什么时候可以到账', '0')\n",
      "('“微粒贷开通\"', '你好，我的微粒贷怎么没有开通呢', '0')\n"
     ]
    }
   ],
   "source": [
    "# 输出训练集的前 3 条样本\n",
    "for idx, example in enumerate(train_ds):\n",
    "    if idx <= 3:\n",
    "        print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 数据预处理\n",
    "\n",
    "通过 PaddleNLP 加载进来的 [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) 数据集是原始的明文数据集，这部分我们来实现组 batch、tokenize 等预处理逻辑，将原始明文数据转换成网络训练的输入数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 定义样本转换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-16 07:57:59,164] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-gram-zh/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# 因为是基于预训练模型 ERNIE-Gram 来进行，所以需要首先加载 ERNIE-Gram 的 tokenizer，\n",
    "# 后续样本转换函数基于 tokenizer 对文本进行切分\n",
    "\n",
    "tokenizer = paddlenlp.transformers.ErnieGramTokenizer.from_pretrained('ernie-gram-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将 1 条明文数据的 query、title 拼接起来，根据预训练模型的 tokenizer 将明文转换为 ID 数据\n",
    "# 返回 input_ids 和 token_type_ids\n",
    "\n",
    "# def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "\n",
    "#     query, title = example[\"query\"], example[\"title\"]\n",
    "\n",
    "#     encoded_inputs = tokenizer(\n",
    "#         text=query, text_pair=title, max_seq_len=max_seq_length)\n",
    "\n",
    "#     input_ids = encoded_inputs[\"input_ids\"]\n",
    "#     token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "#     if not is_test:\n",
    "#         label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "#         return input_ids, token_type_ids, label\n",
    "#     # 在预测或者评估阶段，不返回 label 字段\n",
    "#     else:\n",
    "#         return input_ids, token_type_ids\n",
    "\n",
    "\n",
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "\n",
    "    query, title, _ = example\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=query, text_pair=title, max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[-1]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    # 在预测或者评估阶段，不返回 label 字段\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 对训练集的第 1 条数据进行转换\n",
    "input_ids, token_type_ids, label = convert_example(train_ds[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 702, 212, 566, 453, 559, 1114, 2, 201, 9, 65, 332, 958, 340, 201, 2]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 为了后续方便使用，我们使用python偏函数（partial）给 convert_example 赋予一些默认参数\n",
    "from functools import partial\n",
    "\n",
    "# 训练集和验证集的样本转换函数\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 组装 Batch 数据 & Padding\n",
    "\n",
    "上一小节，我们完成了对单条样本的转换，本节我们需要将样本组合成 Batch 数据，对于不等长的数据还需要进行 Padding 操作，便于 GPU 训练。\n",
    "\n",
    "PaddleNLP 提供了许多关于 NLP 任务中构建有效的数据 pipeline 的常用 API\n",
    "\n",
    "| API                             | 简介                                       |\n",
    "| ------------------------------- | :----------------------------------------- |\n",
    "| `paddlenlp.data.Stack`          | 堆叠N个具有相同shape的输入数据来构建一个batch |\n",
    "| `paddlenlp.data.Pad`            | 将长度不同的多个句子padding到统一长度，取N个输入数据中的最大长度 |\n",
    "| `paddlenlp.data.Tuple`          | 将多个batchify函数包装在一起 |\n",
    "\n",
    "更多数据处理操作详见： [https://paddlenlp.readthedocs.io/zh/latest/data_prepare/data_preprocess.html](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/data_preprocess.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Data: \n",
      " [[1 2 3 4]\n",
      " [3 4 5 6]\n",
      " [5 6 7 8]]\n",
      "\n",
      "Padded Data: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 0]\n",
      " [8 9 0 0]]\n",
      "\n",
      "ids: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 0]\n",
      " [8 9 0 0]]\n",
      "\n",
      "labels: \n",
      " [[1]\n",
      " [0]\n",
      " [1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.data import Stack, Pad, Tuple\n",
    "a = [1, 2, 3, 4]\n",
    "b = [3, 4, 5, 6]\n",
    "c = [5, 6, 7, 8]\n",
    "result = Stack()([a, b, c])\n",
    "print(\"Stacked Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "a = [1, 2, 3, 4]\n",
    "b = [5, 6, 7]\n",
    "c = [8, 9]\n",
    "result = Pad(pad_val=0)([a, b, c])\n",
    "print(\"Padded Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "data = [\n",
    "        [[1, 2, 3, 4], [1]],\n",
    "        [[5, 6, 7], [0]],\n",
    "        [[8, 9], [1]],\n",
    "       ]\n",
    "batchify_fn = Tuple(Pad(pad_val=0), Stack())\n",
    "ids, labels = batchify_fn(data)\n",
    "print(\"ids: \\n\", ids)\n",
    "print()\n",
    "print(\"labels: \\n\", labels)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 我们的训练数据会返回 input_ids, token_type_ids, labels 3 个字段\n",
    "# 因此针对这 3 个字段需要分别定义 3 个组 batch 操作\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 定义 Dataloader\n",
    "下面我们基于组 batchify_fn 函数和样本转换函数 trans_func 来构造训练集的 DataLoader, 支持多卡训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 定义分布式 Sampler: 自动对训练数据进行切分，支持多卡并行训练\n",
    "batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "# 基于 train_ds 定义 train_data_loader\n",
    "# 因为我们使用了分布式的 DistributedBatchSampler, train_data_loader 会自动对训练数据进行切分\n",
    "train_data_loader = paddle.io.DataLoader(\n",
    "        dataset=train_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "# 针对验证集数据加载，我们使用单卡进行评估，所以采用 paddle.io.BatchSampler 即可\n",
    "# 定义 dev_data_loader\n",
    "batch_sampler = paddle.io.BatchSampler(dev_ds, batch_size=32, shuffle=False)\n",
    "dev_data_loader = paddle.io.DataLoader(\n",
    "        dataset=dev_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 模型搭建\n",
    "\n",
    "自从 2018 年 10 月以来，NLP 个领域的任务都通过 Pretrain + Finetune 的模式相比传统 DNN 方法在效果上取得了显著的提升，本节我们以百度开源的预训练模型 ERNIE-Gram 为基础模型，在此之上构建 Point-wise 语义匹配网络。\n",
    "\n",
    "首先我们来定义网络结构:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-16 07:58:57,367] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/ernie_gram_zh.pdparams\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn as nn\n",
    "\n",
    "# 我们基于 ERNIE-Gram 模型结构搭建 Point-wise 语义匹配网络\n",
    "# 所以此处先定义 ERNIE-Gram 的 pretrained_model\n",
    "pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
    "#pretrained_model = paddlenlp.transformers.ErnieModel.from_pretrained('ernie-1.0')\n",
    "\n",
    "\n",
    "class PointwiseMatching(nn.Layer):\n",
    "   \n",
    "    # 此处的 pretained_model 在本例中会被 ERNIE-Gram 预训练模型初始化\n",
    "    def __init__(self, pretrained_model, dropout=None):\n",
    "        super().__init__()\n",
    "        self.ptm = pretrained_model\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "\n",
    "        # 语义匹配任务: 相似、不相似 2 分类任务\n",
    "        self.classifier = nn.Linear(self.ptm.config[\"hidden_size\"], 2)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                attention_mask=None):\n",
    "\n",
    "        # 此处的 Input_ids 由两条文本的 token ids 拼接而成\n",
    "        # token_type_ids 表示两段文本的类型编码\n",
    "        # 返回的 cls_embedding 就表示这两段文本经过模型的计算之后而得到的语义表示向量\n",
    "        _, cls_embedding = self.ptm(input_ids, token_type_ids, position_ids,\n",
    "                                    attention_mask)\n",
    "\n",
    "        cls_embedding = self.dropout(cls_embedding)\n",
    "\n",
    "        # 基于文本对的语义表示向量进行 2 分类任务\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        probs = F.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "# 定义 Point-wise 语义匹配网络\n",
    "model = PointwiseMatching(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 模型训练 & 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "epochs = 3\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(5E-5, num_training_steps, 0.0)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.0,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# 采用交叉熵 损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 因为训练过程中同时要在验证集进行模型评估，因此我们先定义评估函数\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, phase=\"dev\"):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        loss = criterion(probs, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval {} loss: {:.5}, accu: {:.5}\".format(phase,\n",
    "                                                    np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9370, epoch: 3, batch: 3120, loss: 0.38331, accu: 0.89152, speed: 8.58 step/s\r"
     ]
    }
   ],
   "source": [
    "# 接下来，开始正式训练模型，训练时间较长，可注释掉这部分\n",
    "\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        loss = criterion(probs, labels)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "        # 每间隔 10 step 输出训练指标\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        # 每间隔 100 step 在验证集和测试集上进行评估\n",
    "        if global_step % 100 == 0:\n",
    "            evaluate(model, criterion, metric, dev_data_loader, \"dev\")\n",
    "            \n",
    "# 训练结束后，存储模型参数\n",
    "save_dir = os.path.join(\"checkpoint\", \"model_%d\" % global_step)\n",
    "os.makedirs(save_dir)\n",
    "\n",
    "save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "paddle.save(model.state_dict(), save_param_path)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练过程中会输出如下日志:\n",
    "```\n",
    "global step 5310, epoch: 3, batch: 1578, loss: 0.31671, accu: 0.95000, speed: 0.63 step/s\n",
    "global step 5320, epoch: 3, batch: 1588, loss: 0.36240, accu: 0.94063, speed: 6.98 step/s\n",
    "global step 5330, epoch: 3, batch: 1598, loss: 0.41451, accu: 0.93854, speed: 7.40 step/s\n",
    "global step 5340, epoch: 3, batch: 1608, loss: 0.31327, accu: 0.94063, speed: 7.01 step/s\n",
    "global step 5350, epoch: 3, batch: 1618, loss: 0.40664, accu: 0.93563, speed: 7.83 step/s\n",
    "global step 5360, epoch: 3, batch: 1628, loss: 0.33064, accu: 0.93958, speed: 7.34 step/s\n",
    "global step 5370, epoch: 3, batch: 1638, loss: 0.38411, accu: 0.93795, speed: 7.72 step/s\n",
    "global step 5380, epoch: 3, batch: 1648, loss: 0.35376, accu: 0.93906, speed: 7.92 step/s\n",
    "global step 5390, epoch: 3, batch: 1658, loss: 0.39706, accu: 0.93924, speed: 7.47 step/s\n",
    "global step 5400, epoch: 3, batch: 1668, loss: 0.41198, accu: 0.93781, speed: 7.41 step/s\n",
    "eval dev loss: 0.4177, accu: 0.89082\n",
    "global step 5410, epoch: 3, batch: 1678, loss: 0.34453, accu: 0.93125, speed: 0.63 step/s\n",
    "global step 5420, epoch: 3, batch: 1688, loss: 0.34569, accu: 0.93906, speed: 7.75 step/s\n",
    "global step 5430, epoch: 3, batch: 1698, loss: 0.39160, accu: 0.92917, speed: 7.54 step/s\n",
    "global step 5440, epoch: 3, batch: 1708, loss: 0.46002, accu: 0.93125, speed: 7.05 step/s\n",
    "global step 5450, epoch: 3, batch: 1718, loss: 0.32302, accu: 0.93188, speed: 7.14 step/s\n",
    "global step 5460, epoch: 3, batch: 1728, loss: 0.40802, accu: 0.93281, speed: 7.22 step/s\n",
    "global step 5470, epoch: 3, batch: 1738, loss: 0.34607, accu: 0.93348, speed: 7.44 step/s\n",
    "global step 5480, epoch: 3, batch: 1748, loss: 0.34709, accu: 0.93398, speed: 7.38 step/s\n",
    "global step 5490, epoch: 3, batch: 1758, loss: 0.31814, accu: 0.93437, speed: 7.39 step/s\n",
    "global step 5500, epoch: 3, batch: 1768, loss: 0.42689, accu: 0.93125, speed: 7.74 step/s\n",
    "eval dev loss: 0.41789, accu: 0.88968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "基于默认参数配置进行单卡训练大概要持续 4 个小时左右，会训练完成 3 个 Epoch, 模型最终的收敛指标结果如下:\n",
    "\n",
    "\n",
    "| 数据集 | Accuracy |\n",
    "| -------- | -------- |\n",
    "| dev.tsv     | 89.62  |\n",
    "\n",
    "可以看到: 我们基于 PaddleNLP ，利用 ERNIE-Gram 预训练模型使用非常简洁的代码，就在权威语义匹配数据集上取得了很不错的效果."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.5 模型预测\n",
    "\n",
    "接下来我们使用已经训练好的语义匹配模型对一些预测数据进行预测。待预测数据为每行都是文本对的 tsv 文件，我们使用 Lcqmc 数据集的测试集作为我们的预测数据，进行预测并提交预测结果到 [千言文本相似度竞赛](https://aistudio.baidu.com/aistudio/competition/detail/45)\n",
    "\n",
    "下载我们已经训练好的语义匹配模型, 并解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-16 07:22:00--  https://paddlenlp.bj.bcebos.com/models/text_matching/ernie_gram_zh_pointwise_matching_model.tar\n",
      "Resolving paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)... 182.61.200.195, 182.61.200.229, 2409:8c00:6c21:10ad:0:ff:b00e:67d\n",
      "Connecting to paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)|182.61.200.195|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 597667840 (570M) [application/x-tar]\n",
      "Saving to: ‘ernie_gram_zh_pointwise_matching_model.tar.2’\n",
      "\n",
      "ernie_gram_zh_point 100%[===================>] 569.98M  41.1MB/s    in 14s     \n",
      "\n",
      "2021-06-16 07:22:14 (40.3 MB/s) - ‘ernie_gram_zh_pointwise_matching_model.tar.2’ saved [597667840/597667840]\n",
      "\n",
      "ernie_gram_zh_pointwise_matching_model/\n",
      "ernie_gram_zh_pointwise_matching_model/model_state.pdparams\n",
      "ernie_gram_zh_pointwise_matching_model/vocab.txt\n",
      "ernie_gram_zh_pointwise_matching_model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# 下载我们基于 Lcqmc 事先训练好的语义匹配模型并解压\n",
    "! wget https://paddlenlp.bj.bcebos.com/models/text_matching/ernie_gram_zh_pointwise_matching_model.tar\n",
    "! tar -xvf ernie_gram_zh_pointwise_matching_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "谁有狂三这张高清的\t这张高清图，谁有\r\n",
      "英雄联盟什么英雄最好\t英雄联盟最好英雄是什么\r\n",
      "这是什么意思，被蹭网吗\t我也是醉了，这是什么意思\r\n"
     ]
    }
   ],
   "source": [
    "# 测试数据由 2 列文本构成 tab 分隔\n",
    "# Lcqmc 默认下载到如下路径\n",
    "! head -n3 \"${HOME}/.paddlenlp/datasets/LCQMC/lcqmc/lcqmc/test.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 定义预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, data_loader):\n",
    "    \n",
    "    batch_probs = []\n",
    "\n",
    "    # 预测阶段打开 eval 模式，模型中的 dropout 等操作会关掉\n",
    "    model.eval()\n",
    "\n",
    "    with paddle.no_grad():\n",
    "        for batch_data in data_loader:\n",
    "            input_ids, token_type_ids = batch_data\n",
    "            input_ids = paddle.to_tensor(input_ids)\n",
    "            token_type_ids = paddle.to_tensor(token_type_ids)\n",
    "            \n",
    "            # 获取每个样本的预测概率: [batch_size, 2] 的矩阵\n",
    "            batch_prob = model(\n",
    "                input_ids=input_ids, token_type_ids=token_type_ids).numpy()\n",
    "\n",
    "            batch_probs.append(batch_prob)\n",
    "        batch_probs = np.concatenate(batch_probs, axis=0)\n",
    "\n",
    "        return batch_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 定义预测数据的 data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预测数据的转换函数\n",
    "# predict 数据没有 label, 因此 convert_exmaple 的 is_test 参数设为 True\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    is_test=True)\n",
    "\n",
    "# 预测数据的组 batch 操作\n",
    "# predict 数据只返回 input_ids 和 token_type_ids，因此只需要 2 个 Pad 对象作为 batchify_fn\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment_ids\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "# 加载预测数据\n",
    "# test_ds = load_dataset(\"lcqmc\", splits=[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# 生成预测数据 data_loader\n",
    "predict_data_loader =paddle.io.DataLoader(\n",
    "        dataset=test_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 定义预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 选择预训练ernie gram，填写自己的代码\n",
    "pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
    "\n",
    "model = PointwiseMatching(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 加载已训练好的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 刚才下载的模型解压之后存储路径为 ./ernie_gram_zh_pointwise_matching_model/model_state.pdparams\n",
    "state_dict = paddle.load(\"./ernie_gram_zh_pointwise_matching_model/model_state.pdparams\")\n",
    "\n",
    "# 刚才下载的模型解压之后存储路径为 ./pointwise_matching_model/ernie1.0_base_pointwise_matching.pdparams\n",
    "# state_dict = paddle.load(\"pointwise_matching_model/ernie1.0_base_pointwise_matching.pdparams\")\n",
    "model.set_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 开始预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[32, 48], dtype=int64, place=CUDAPinnedPlace, stop_gradient=True,\n",
      "       [[1   , 13  , 3221, ..., 0   , 0   , 0   ],\n",
      "        [1   , 1051, 15  , ..., 0   , 0   , 0   ],\n",
      "        [1   , 13  , 614 , ..., 0   , 0   , 0   ],\n",
      "        ...,\n",
      "        [1   , 16  , 1051, ..., 0   , 0   , 0   ],\n",
      "        [1   , 413 , 41  , ..., 0   , 0   , 0   ],\n",
      "        [1   , 16  , 96  , ..., 0   , 0   , 0   ]]), Tensor(shape=[32, 48], dtype=int64, place=CUDAPinnedPlace, stop_gradient=True,\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(predict_data_loader):\n",
    "    if idx < 1:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 执行预测函数\n",
    "y_probs = predict(model, predict_data_loader)\n",
    "\n",
    "# 根据预测概率获取预测 label\n",
    "y_preds = np.argmax(y_probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 输出预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 13, 3221, 806, 16, 15, 2, 13, 240, 218, 201, 165, 160, 496, 603, 1118, 1091, 2], 0]\n",
      "[[1, 1051, 15, 958, 4, 255, 201, 340, 9, 124, 93, 4, 48, 22, 370, 566, 1114, 12045, 2, 48, 955, 370, 566, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 938, 647, 852, 130, 789, 109, 632, 1086, 113, 7, 339, 10, 312, 27, 1619, 76, 2, 13, 614, 356, 41, 789, 109, 414, 1046, 632, 1086, 61, 412, 406, 2], 0]\n",
      "[[1, 508, 125, 1051, 181, 125, 201, 699, 48, 22, 12045, 2, 508, 125, 1051, 181, 125, 201, 41, 247, 607, 453, 1114, 2], 0]\n",
      "[[1, 226, 170, 12044, 508, 86, 1598, 2992, 340, 9, 1834, 75, 699, 12045, 2, 226, 170, 508, 125, 936, 356, 340, 9, 1834, 699, 1091, 2], 0]\n",
      "[[1, 110, 1051, 5, 958, 10, 955, 48, 22, 160, 87, 12045, 2, 310, 872, 1051, 699, 48, 22, 160, 87, 1114, 12044, 2], 1]\n",
      "[[1, 16, 10, 1577, 647, 5, 478, 8, 113, 16, 52, 1051, 699, 1114, 2, 7, 689, 614, 356, 314, 116, 8, 32, 295, 1577, 647, 2], 0]\n",
      "[[1, 8, 1529, 603, 348, 16, 15, 4, 88, 16, 15, 246, 2, 75, 789, 109, 5, 138, 402, 165, 10, 243, 524, 5, 4, 13, 614, 356, 238, 178, 16, 1092, 88, 246, 603, 1118, 12045, 2], 1]\n",
      "[[1, 7, 125, 217, 399, 170, 65, 958, 2, 208, 211, 217, 399, 7, 125, 10, 317, 183, 10, 1114, 2], 1]\n",
      "[[1, 13, 3221, 201, 340, 88, 364, 1260, 2, 16, 10, 702, 1216, 1531, 478, 246, 4, 936, 356, 48, 22, 295, 1577, 2], 1]\n",
      "[[1, 88, 124, 42, 702, 1216, 1531, 2, 836, 75, 88, 124, 2], 1]\n",
      "[[1, 4376, 356, 424, 52, 185, 45, 1577, 647, 2, 13, 614, 356, 75, 515, 500, 1834, 201, 699, 5, 45, 87, 11, 201, 340, 2], 1]\n",
      "[[1, 1834, 699, 36, 143, 10, 553, 180, 2, 154, 217, 399, 5, 201, 699, 36, 143, 10, 65, 876, 12045, 2], 0]\n",
      "[[1, 13, 614, 356, 1051, 699, 238, 10, 16, 124, 93, 2, 13, 614, 356, 598, 647, 16, 124, 93, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 5, 154, 72, 459, 335, 852, 130, 2, 13, 614, 356, 75, 180, 71, 299, 340, 9, 852, 130, 1091, 2], 0]\n",
      "[[1, 647, 358, 136, 399, 65, 332, 2, 249, 211, 949, 136, 399, 10, 65, 332, 2], 1]\n",
      "[[1, 1051, 958, 48, 170, 370, 87, 2, 48, 22, 160, 87, 192, 12045, 2], 1]\n",
      "[[1, 48, 22, 88, 215, 334, 406, 181, 1114, 12045, 2, 201, 334, 958, 49, 52, 801, 607, 1051, 1114, 12045, 2], 1]\n",
      "[[1, 226, 170, 4, 75, 797, 40, 777, 171, 72, 245, 898, 15, 4, 52, 955, 806, 75, 464, 85, 5, 777, 201, 699, 2, 806, 777, 614, 356, 36, 717, 52, 806, 170, 2], 1]\n",
      "[[1, 7, 689, 10, 614, 356, 36, 717, 67, 70, 1834, 699, 12045, 2, 7, 689, 553, 180, 1834, 699, 2], 1]\n",
      "[[1, 75, 313, 358, 614, 356, 36, 717, 32, 486, 445, 128, 543, 93, 61, 1091, 12045, 2, 312, 614, 356, 36, 717, 445, 61, 128, 543, 12045, 2], 1]\n",
      "[[1, 647, 358, 9, 47, 27, 247, 98, 732, 5, 3887, 1114, 2, 247, 98, 212, 500, 16, 170, 2], 1]\n",
      "[[1, 88, 16, 15, 246, 2, 8, 1529, 603, 348, 16, 15, 4, 88, 16, 15, 246, 2], 0]\n",
      "[[1, 16, 596, 581, 702, 539, 797, 40, 304, 259, 2, 23, 892, 556, 596, 581, 702, 539, 797, 40, 598, 650, 41, 323, 4, 154, 72, 459, 335, 852, 130, 24, 4, 47, 10, 13, 614, 356, 12045, 614, 356, 250, 196, 1091, 2], 0]\n",
      "[[1, 312, 75, 1082, 1082, 938, 647, 5, 1531, 699, 12043, 12043, 370, 566, 1407, 936, 356, 868, 25, 2, 1082, 1082, 938, 647, 15, 1531, 699, 4, 48, 22, 370, 566, 1114, 12045, 2], 0]\n",
      "[[1, 614, 356, 36, 717, 34, 39, 1577, 647, 1091, 2, 1051, 699, 340, 445, 128, 543, 2], 0]\n",
      "[[1, 13, 614, 356, 160, 152, 201, 334, 110, 9, 1051, 699, 16, 52, 486, 1051, 1091, 12045, 2, 160, 152, 201, 699, 217, 399, 936, 356, 559, 2], 1]\n",
      "[[1, 75, 806, 247, 98, 500, 15, 2, 142, 228, 75, 806, 247, 98, 936, 356, 315, 12045, 2], 0]\n",
      "[[1, 13, 614, 356, 1082, 1082, 1051, 958, 41, 789, 109, 321, 65, 218, 412, 406, 1086, 14, 414, 1046, 632, 1086, 10, 16, 10, 75, 247, 98, 358, 281, 201, 699, 113, 48, 22, 2, 1051, 699, 538, 607, 789, 412, 406, 1086, 14, 632, 1086, 2], 1]\n",
      "[[1, 16, 1051, 15, 16, 41, 445, 61, 15, 2, 16, 313, 1051, 958, 15, 4, 255, 10, 16, 96, 111, 588, 45, 1051, 958, 588, 2742, 15, 2], 1]\n",
      "[[1, 413, 41, 160, 353, 614, 356, 1051, 699, 516, 402, 2, 10, 16, 10, 2502, 228, 247, 98, 165, 340, 88, 124, 1260, 2], 1]\n",
      "[[1, 16, 96, 111, 1507, 533, 2, 16, 170, 221, 461, 4, 1082, 1082, 9, 104, 340, 9, 293, 818, 892, 5, 128, 543, 4, 87, 11, 48, 22, 445, 93, 61, 15, 4, 1183, 1183, 12044, 2], 0]\n",
      "[[1, 936, 356, 129, 1407, 47, 27, 2, 142, 449, 917, 39, 702, 217, 1531, 2], 1]\n",
      "[[1, 3384, 155, 5, 702, 1216, 1531, 936, 356, 828, 16, 45, 15, 2, 429, 275, 11, 1256, 464, 2], 1]\n",
      "[[1, 1263, 500, 1834, 699, 4, 508, 125, 936, 356, 340, 9, 1834, 2, 892, 170, 4, 508, 125, 75, 340, 9, 1834, 699, 4, 226, 459, 459, 2], 0]\n",
      "[[1, 601, 4, 12, 468, 5, 41, 936, 314, 721, 31, 1392, 1091, 12045, 2, 41, 936, 314, 684, 552, 12045, 2], 1]\n",
      "[[1, 13, 614, 356, 2623, 1051, 699, 556, 124, 93, 2, 75, 13, 614, 356, 1051, 16, 200, 61, 2], 0]\n",
      "[[1, 3384, 958, 392, 340, 9, 109, 270, 10, 16, 10, 340, 2717, 91, 797, 40, 777, 2, 3384, 702, 1216, 1531, 14, 702, 212, 702, 1216, 1531, 9, 614, 356, 16, 7, 314, 2], 1]\n",
      "[[1, 4376, 356, 88, 124, 2, 48, 22, 938, 647, 1114, 340, 9, 702, 1216, 1531, 2226, 500, 2], 1]\n",
      "[[1, 142, 449, 263, 285, 247, 98, 500, 2, 142, 449, 263, 285, 2717, 91, 5, 247, 98, 500, 1086, 12045, 2], 1]\n",
      "[[1, 13, 614, 356, 32, 938, 647, 603, 1118, 12045, 2, 255, 10, 598, 592, 16, 124, 93, 2], 0]\n",
      "[[1, 142, 449, 273, 2717, 797, 40, 777, 2, 142, 228, 273, 599, 797, 40, 777, 2717, 91, 2], 0]\n",
      "[[1, 936, 356, 743, 564, 1051, 699, 852, 130, 2, 852, 130, 743, 564, 15, 2], 0]\n",
      "[[1, 892, 170, 647, 427, 1051, 699, 369, 52, 129, 1459, 2, 16, 41, 113, 178, 47, 27, 2], 1]\n",
      "[[1, 1051, 699, 36, 4, 797, 40, 412, 406, 128, 543, 340, 293, 45, 2, 340, 293, 45, 128, 543, 524, 418, 12044, 614, 356, 36, 717, 11, 445, 2], 1]\n",
      "[[1, 1051, 958, 15, 936, 356, 201, 1260, 2, 1243, 500, 201, 699, 139, 48, 22, 16, 1853, 1947, 1834, 699, 4, 75, 1243, 139, 86, 1598, 201, 699, 48, 22, 1114, 2], 1]\n",
      "[[1, 226, 170, 4, 75, 2717, 91, 5, 797, 40, 777, 1407, 15, 4, 52, 806, 93, 7, 501, 797, 40, 777, 2717, 91, 1114, 2, 2717, 91, 5, 797, 40, 777, 2328, 15, 936, 356, 315, 806, 777, 806, 1834, 699, 777, 2], 1]\n",
      "[[1, 247, 98, 500, 806, 15, 4, 936, 356, 263, 285, 2, 75, 5, 247, 98, 500, 1086, 806, 15, 936, 356, 806, 12045, 2], 0]\n",
      "[[1, 142, 449, 160, 69, 1051, 699, 852, 130, 2, 540, 42, 2802, 12054, 10, 65, 2], 1]\n",
      "[[1, 75, 13, 614, 356, 619, 9, 852, 130, 2, 13, 614, 356, 75, 9, 1132, 667, 11, 47, 155, 340, 9, 852, 130, 12045, 2], 1]\n",
      "[[1, 75, 48, 22, 57, 70, 416, 797, 40, 445, 128, 543, 61, 524, 418, 1114, 2, 938, 647, 65, 332, 852, 130, 413, 41, 128, 543, 524, 418, 2], 0]\n",
      "[[1, 142, 449, 273, 599, 2, 936, 356, 314, 424, 52, 427, 702, 1216, 1531, 2654, 1407, 2], 1]\n",
      "[[1, 540, 88, 124, 2, 88, 124, 6, 1051, 699, 2], 1]\n",
      "[[1, 48, 22, 490, 579, 702, 539, 797, 40, 265, 60, 88, 246, 5, 15, 12045, 2, 16, 41, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 777, 9, 581, 824, 958, 4, 255, 201, 699, 603, 1118, 12045, 2, 13, 614, 356, 2226, 246, 9, 958, 4, 75, 160, 152, 201, 699, 4, 535, 597, 496, 201, 699, 603, 1118, 12044, 2], 0]\n",
      "[[1, 28, 76, 160, 496, 75, 556, 596, 581, 1051, 1531, 304, 259, 2, 16, 1092, 95, 702, 797, 40, 5, 304, 259, 2], 1]\n",
      "[[1, 65, 872, 1051, 1531, 217, 399, 2, 397, 12049, 936, 356, 159, 399, 2], 1]\n",
      "[[1, 6, 936, 356, 160, 852, 2, 88, 124, 936, 356, 88, 151, 2], 1]\n",
      "[[1, 598, 592, 413, 41, 65, 84, 36, 143, 12045, 2, 1051, 699, 413, 41, 65, 84, 36, 143, 598, 592, 2], 0]\n",
      "[[1, 142, 228, 349, 116, 438, 577, 2, 75, 41, 71, 109, 3156, 1531, 2], 1]\n",
      "[[1, 556, 596, 581, 797, 40, 598, 650, 2, 142, 449, 596, 581, 598, 650, 41, 323, 2], 0]\n",
      "[[1, 3384, 28, 105, 597, 496, 1051, 699, 603, 1118, 2, 28, 76, 75, 52, 1051, 249, 608, 317, 12043, 87, 11, 1051, 15, 317, 502, 4, 201, 9, 249, 608, 1051, 16, 39, 61, 15, 12045, 13, 614, 356, 12045, 2], 1]\n",
      "[[1, 247, 98, 500, 806, 15, 4, 936, 356, 263, 285, 2, 936, 314, 263, 285, 128, 543, 500, 1086, 2], 0]\n",
      "[[1, 129, 1459, 702, 1216, 850, 2, 340, 185, 45, 212, 399, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 1051, 5, 958, 201, 340, 45, 1915, 12045, 2, 7, 689, 65, 84, 36, 143, 45, 1915, 2], 1]\n",
      "[[1, 10, 16, 10, 2502, 228, 98, 85, 597, 496, 16, 15, 2, 567, 766, 10, 826, 52, 247, 98, 12045, 2], 1]\n",
      "[[1, 13, 596, 581, 702, 539, 797, 40, 598, 650, 41, 323, 10, 614, 356, 221, 461, 2, 13, 614, 356, 75, 16, 52, 29, 2], 1]\n",
      "[[1, 702, 1216, 1531, 1051, 958, 1619, 76, 597, 496, 556, 596, 581, 702, 539, 797, 40, 598, 650, 2, 194, 26, 797, 40, 2], 1]\n",
      "[[1, 1082, 424, 75, 789, 990, 15, 262, 793, 412, 406, 4, 215, 228, 113, 47, 314, 15, 2, 226, 170, 75, 313, 1051, 699, 2876, 125, 789, 109, 632, 1086, 990, 1268, 255, 10, 508, 125, 7, 339, 597, 496, 75, 632, 1086, 789, 109, 990, 1268, 218, 179, 93, 65, 41, 131, 177, 125, 1679, 525, 2], 0]\n",
      "[[1, 178, 262, 793, 406, 500, 1086, 65, 218, 789, 990, 154, 72, 1051, 699, 2, 75, 265, 60, 427, 384, 524, 5, 262, 793, 406, 500, 1086, 789, 71, 299, 15, 4, 13, 614, 356, 16, 48, 22, 1051, 1531, 2453, 2], 0]\n",
      "[[1, 3384, 155, 76, 340, 9, 702, 1216, 256, 936, 356, 315, 2, 255, 10, 3384, 340, 9, 2], 0]\n",
      "[[1, 75, 9, 312, 309, 556, 596, 581, 797, 40, 598, 650, 2, 556, 596, 581, 598, 650, 2], 1]\n",
      "[[1, 132, 852, 340, 9, 2, 48, 22, 1577, 647, 75, 16, 12045, 416, 75, 27, 132, 852, 2], 1]\n",
      "[[1, 936, 356, 424, 52, 559, 10, 1577, 647, 1132, 667, 2, 16, 10, 1577, 18017, 5, 6, 41, 936, 356, 314, 88, 124, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 11, 247, 98, 28, 445, 16, 88, 702, 1216, 1531, 2, 13, 614, 356, 348, 8, 247, 98, 9, 2], 1]\n",
      "[[1, 1082, 1082, 16, 96, 111, 1507, 1407, 15, 226, 115, 445, 93, 61, 5, 128, 543, 15, 2, 990, 93, 598, 592, 128, 543, 2], 1]\n",
      "[[1, 556, 596, 581, 37, 702, 539, 797, 40, 598, 650, 2, 556, 596, 581, 938, 647, 41, 323, 10, 614, 356, 2], 0]\n",
      "[[1, 2008, 201, 699, 553, 125, 32, 347, 639, 212, 29, 2, 128, 543, 524, 418, 413, 41, 65, 876, 12045, 2], 1]\n",
      "[[1, 13, 614, 356, 128, 543, 598, 592, 16, 124, 93, 2, 1082, 1082, 598, 592, 15, 4, 936, 356, 556, 124, 93, 12045, 2], 1]\n",
      "[[1, 16, 1092, 95, 1051, 699, 41, 323, 2, 16, 596, 581, 598, 592, 2], 1]\n",
      "[[1, 597, 496, 75, 262, 793, 789, 109, 990, 1268, 65, 4, 154, 72, 1051, 699, 2, 75, 1051, 699, 16, 33, 369, 936, 356, 27, 182, 617, 12045, 2], 0]\n",
      "[[1, 1051, 699, 48, 22, 1051, 553, 218, 2, 7, 8, 48, 22, 1051, 553, 218, 12045, 2], 1]\n",
      "[[1, 44, 178, 75, 262, 793, 212, 399, 789, 109, 990, 1268, 218, 179, 93, 65, 936, 356, 1918, 2, 13, 614, 356, 807, 684, 212, 399, 1918, 16, 15, 2], 0]\n",
      "[[1, 142, 449, 349, 116, 702, 1216, 1531, 2, 1051, 15, 699, 4, 340, 597, 496, 2], 1]\n",
      "[[1, 226, 170, 702, 1216, 1531, 938, 647, 413, 41, 312, 309, 138, 402, 5, 15, 12043, 2, 226, 170, 4, 75, 247, 98, 28, 936, 356, 597, 496, 702, 1216, 1531, 201, 340, 9, 88, 124, 2], 1]\n",
      "[[1, 52, 16, 52, 836, 75, 459, 7, 86, 75, 47, 27, 136, 5, 1915, 269, 10, 65, 332, 2, 1051, 27, 136, 59, 195, 201, 1915, 269, 2], 1]\n",
      "[[1, 9, 128, 543, 75, 340, 293, 45, 340, 104, 940, 2, 1082, 598, 592, 128, 543, 340, 9, 293, 45, 2], 1]\n",
      "[[1, 13, 614, 356, 480, 1349, 556, 124, 93, 2, 13, 614, 356, 9, 15, 702, 1216, 1531, 280, 76, 598, 592, 16, 124, 93, 2], 0]\n",
      "[[1, 340, 9, 124, 93, 936, 356, 315, 2, 13, 614, 356, 75, 16, 596, 581, 702, 539, 797, 40, 5, 598, 650, 275, 378, 12045, 275, 378, 10, 614, 356, 1091, 2], 1]\n",
      "[[1, 938, 647, 998, 195, 7, 125, 201, 699, 2, 2556, 195, 208, 125, 32, 374, 109, 27, 8, 725, 212, 1114, 12045, 2], 1]\n",
      "[[1, 3345, 845, 59, 317, 27, 136, 4, 208, 27, 136, 201, 328, 936, 356, 159, 399, 2, 589, 125, 103, 16, 159, 399, 4, 936, 273, 12045, 2], 0]\n",
      "[[1, 226, 170, 4, 75, 201, 699, 2226, 282, 263, 41, 142, 449, 263, 285, 2, 936, 356, 263, 285, 59, 195, 201, 699, 2], 0]\n",
      "[[1, 1051, 699, 49, 797, 40, 340, 61, 128, 543, 936, 356, 370, 566, 2, 16, 29, 160, 1519, 15, 75, 16, 1051, 15, 2], 1]\n",
      "[[1, 1082, 424, 340, 293, 45, 128, 543, 4, 381, 1326, 16, 33, 369, 2, 340, 293, 45, 128, 543, 524, 418, 12044, 614, 356, 36, 717, 11, 445, 2], 1]\n",
      "[[1, 1010, 7, 125, 201, 699, 32, 2556, 195, 1114, 2, 2556, 195, 7, 125, 15, 4, 75, 310, 936, 356, 201, 12045, 2], 1]\n",
      "[[1, 142, 228, 2556, 195, 15, 4, 217, 620, 936, 356, 559, 2, 142, 228, 1051, 15, 1755, 183, 2556, 195, 167, 125, 2], 0]\n",
      "[[1, 11, 340, 2, 45, 87, 11, 105, 340, 45, 1915, 1260, 2], 0]\n",
      "[[1, 89, 195, 201, 699, 374, 704, 2, 75, 201, 699, 374, 704, 48, 22, 2], 0]\n",
      "[[1, 75, 10, 13, 15, 47, 27, 480, 59, 5, 2, 178, 480, 59, 16, 581, 2], 1]\n",
      "[[1, 13, 614, 356, 9, 105, 459, 335, 16, 15, 852, 130, 2, 1344, 5, 4, 13, 614, 356, 75, 340, 9, 852, 130, 2], 0]\n",
      "[[1, 201, 699, 406, 181, 2, 201, 699, 374, 704, 48, 22, 445, 749, 1361, 406, 1114, 2], 0]\n",
      "[[1, 13, 614, 356, 340, 9, 445, 128, 543, 61, 1091, 12045, 2, 160, 276, 15, 1051, 1531, 65, 332, 36, 143, 103, 445, 128, 543, 524, 418, 2], 1]\n",
      "[[1, 936, 356, 87, 11, 16, 52, 201, 699, 2, 87, 11, 196, 13, 138, 192, 358, 281, 154, 72, 107, 36, 201, 699, 2], 0]\n",
      "[[1, 10, 16, 10, 2494, 699, 62, 64, 201, 71, 299, 4, 466, 28, 113, 48, 22, 1531, 39, 61, 2, 142, 228, 160, 152, 201, 334, 48, 22, 486, 1051, 356, 2], 0]\n",
      "[[1, 936, 356, 75, 340, 702, 1216, 1531, 2, 936, 356, 3384, 958, 392, 155, 340, 702, 1216, 1531, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 702, 212, 340, 9, 702, 1216, 1531, 1051, 699, 2, 226, 170, 75, 5, 702, 212, 936, 356, 340, 9, 702, 1216, 1531, 699, 2], 0]\n",
      "[[1, 201, 699, 22, 49, 16, 52, 1051, 15, 4, 614, 356, 182, 617, 2, 87, 11, 13, 449, 16, 52, 201, 958, 12047, 2], 1]\n",
      "[[1, 2502, 228, 247, 98, 52, 88, 124, 1114, 2, 938, 647, 49, 4, 413, 41, 128, 543, 524, 418, 4, 413, 41, 312, 309, 138, 402, 1114, 2], 1]\n",
      "[[1, 91, 195, 476, 699, 66, 100, 2, 91, 195, 476, 699, 852, 130, 2], 0]\n",
      "[[1, 41, 936, 356, 424, 52, 1092, 95, 598, 650, 2, 598, 650, 16, 581, 149, 310, 936, 356, 315, 2], 0]\n",
      "[[1, 217, 399, 936, 356, 159, 559, 2, 226, 170, 4, 47, 27, 936, 356, 559, 217, 399, 12045, 52, 1531, 65, 332, 12045, 2], 1]\n",
      "[[1, 2494, 699, 7, 218, 92, 201, 334, 2, 52, 201, 7, 195, 4, 187, 49, 62, 64, 201, 334, 1114, 2], 0]\n",
      "[[1, 947, 90, 360, 88, 124, 58, 72, 2, 702, 1216, 1531, 936, 356, 938, 647, 2], 1]\n",
      "[[1, 553, 500, 201, 699, 2, 240, 872, 1051, 699, 201, 699, 139, 156, 97, 1114, 2], 0]\n",
      "[[1, 34, 1915, 269, 45, 75, 3384, 2, 226, 170, 52, 459, 7, 86, 75, 5, 2226, 269, 1114, 2], 0]\n",
      "[[1, 702, 1216, 140, 936, 356, 559, 399, 2, 702, 217, 1531, 936, 356, 374, 399, 2], 1]\n",
      "[[1, 414, 303, 1256, 309, 797, 40, 201, 699, 2, 414, 303, 1256, 309, 797, 40, 2], 0]\n",
      "[[1, 226, 170, 12044, 13, 614, 356, 201, 699, 33, 369, 49, 4, 340, 9, 1548, 441, 852, 130, 2, 7, 218, 92, 201, 699, 22, 49, 201, 52, 486, 1051, 699, 1114, 12045, 2], 0]\n",
      "[[1, 1051, 699, 45, 201, 699, 195, 577, 2, 226, 115, 47, 27, 1531, 699, 195, 577, 10, 65, 876, 2], 0]\n",
      "[[1, 226, 170, 75, 41, 1057, 806, 2717, 91, 797, 40, 777, 2, 194, 40, 2717, 91, 52, 315, 1114, 2], 1]\n",
      "[[1, 13, 614, 356, 852, 130, 340, 9, 1548, 441, 1091, 2, 7, 872, 699, 340, 9, 201, 334, 4, 486, 1051, 5, 543, 48, 22, 16, 2], 0]\n",
      "[[1, 142, 449, 1531, 699, 12045, 2, 702, 1216, 1531, 201, 699, 702, 1216, 1531, 2], 1]\n",
      "[[1, 48, 22, 938, 647, 426, 2008, 201, 699, 1114, 2, 142, 228, 75, 413, 41, 998, 2008, 7, 125, 201, 699, 4, 48, 22, 356, 12045, 2], 1]\n",
      "[[1, 226, 170, 75, 936, 356, 238, 10, 201, 699, 603, 1118, 2, 508, 125, 75, 41, 201, 699, 6897, 12043, 75, 5, 777, 155, 958, 824, 4, 13, 614, 356, 1834, 699, 603, 1118, 2], 0]\n",
      "[[1, 13, 614, 356, 16, 52, 339, 293, 2717, 91, 702, 212, 414, 1046, 12045, 12045, 2, 158, 380, 40, 442, 1915, 185, 453, 1114, 12045, 2], 1]\n",
      "[[1, 9998, 1703, 817, 512, 1068, 15, 2, 226, 115, 297, 860, 51, 2502, 228, 247, 98, 29, 246, 1114, 2], 0]\n",
      "[[1, 556, 596, 581, 797, 40, 598, 592, 41, 323, 2, 226, 556, 596, 581, 702, 19, 797, 40, 5, 598, 650, 2], 0]\n",
      "[[1, 1051, 1531, 958, 16, 48, 22, 29, 37, 614, 356, 12045, 2, 1051, 699, 49, 29, 1116, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 5, 1051, 699, 7, 339, 340, 9, 566, 399, 2, 13, 614, 356, 265, 60, 201, 15, 4, 201, 11, 34, 1530, 259, 2], 0]\n",
      "[[1, 614, 356, 36, 717, 52, 1051, 2, 449, 36, 52, 1051, 1531, 1260, 2], 1]\n",
      "[[1, 226, 170, 12044, 75, 313, 938, 647, 1051, 699, 4, 255, 1619, 76, 597, 496, 178, 23, 791, 95, 480, 1349, 556, 124, 93, 24, 4, 647, 358, 10, 614, 356, 250, 196, 12045, 2, 13, 614, 356, 46, 152, 48, 22, 1051, 699, 5, 4, 87, 11, 282, 33, 791, 95, 480, 91, 556, 124, 93, 2], 0]\n",
      "[[1, 226, 170, 75, 313, 264, 151, 1051, 958, 46, 49, 217, 399, 936, 356, 559, 2, 249, 211, 936, 356, 559, 217, 399, 2], 1]\n",
      "[[1, 556, 596, 581, 702, 539, 797, 40, 41, 323, 10, 614, 356, 221, 461, 2, 335, 16, 45, 852, 130, 2], 1]\n",
      "[[1, 313, 201, 699, 4, 238, 10, 16, 52, 445, 88, 2, 173, 152, 154, 72, 201, 699, 4, 10, 75, 1239, 7, 5, 526, 281, 2], 1]\n",
      "[[1, 142, 449, 51, 270, 797, 40, 201, 699, 12045, 2, 508, 125, 134, 49, 7, 125, 201, 699, 36, 143, 12043, 2717, 91, 9318, 9486, 47, 501, 777, 16, 373, 15, 4, 75, 311, 253, 797, 40, 938, 647, 102, 777, 2259, 9481, 1235, 500, 12043, 87, 11, 47, 356, 201, 699, 1091, 12045, 2], 0]\n",
      "[[1, 59, 195, 47, 195, 1915, 269, 2, 836, 75, 34, 7, 86, 2226, 269, 2], 1]\n",
      "[[1, 13, 614, 356, 341, 218, 1531, 699, 165, 41, 185, 370, 453, 29, 2, 348, 5, 614, 356, 453, 29, 10, 955, 165, 340, 9, 15, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 3384, 28, 5, 702, 1216, 1531, 9, 852, 130, 4, 1051, 699, 535, 597, 496, 791, 95, 480, 1349, 556, 124, 93, 12045, 2, 13, 614, 356, 1051, 699, 238, 10, 16, 124, 93, 2], 0]\n",
      "[[1, 459, 335, 16, 45, 75, 48, 1531, 852, 130, 2, 936, 356, 938, 647, 1531, 699, 12045, 2], 1]\n",
      "[[1, 180, 273, 75, 29, 35, 278, 201, 16, 45, 958, 2, 180, 273, 75, 27, 124, 93, 800, 4, 138, 192, 556, 45, 2], 1]\n",
      "[[1, 13, 614, 356, 16, 48, 22, 247, 70, 201, 699, 2, 13, 614, 356, 16, 52, 94, 521, 269, 872, 247, 70, 201, 699, 12045, 2], 0]\n",
      "[[1, 13, 3221, 75, 702, 212, 340, 47, 27, 702, 1216, 1531, 1051, 958, 149, 29, 1091, 2, 3384, 702, 1216, 1531, 1057, 702, 212, 10, 16, 10, 7, 314, 5, 12045, 2], 1]\n",
      "[[1, 702, 539, 797, 40, 4, 341, 125, 442, 39, 9, 577, 852, 1114, 12045, 2, 702, 539, 138, 192, 442, 109, 797, 40, 777, 9, 577, 852, 1114, 2], 0]\n",
      "[[1, 442, 1915, 39, 299, 10, 955, 185, 453, 2, 442, 109, 442, 239, 185, 453, 1114, 2], 0]\n",
      "[[1, 13, 596, 581, 598, 650, 41, 323, 4, 10, 614, 356, 221, 461, 2, 13, 614, 356, 75, 597, 496, 154, 72, 1051, 699, 2], 0]\n",
      "[[1, 226, 170, 7, 339, 597, 496, 262, 793, 406, 789, 109, 990, 1268, 936, 356, 315, 2, 75, 1051, 699, 36, 717, 427, 262, 793, 406, 212, 399, 789, 109, 990, 15, 6, 2], 1]\n",
      "[[1, 16, 48, 22, 160, 87, 1114, 2, 16, 52, 160, 87, 1114, 2], 1]\n",
      "[[1, 65, 218, 1051, 699, 2, 614, 356, 36, 717, 75, 5, 702, 1216, 1531, 48, 22, 88, 364, 2], 0]\n",
      "[[1, 614, 356, 36, 717, 88, 440, 201, 699, 12045, 2, 226, 170, 358, 86, 1051, 699, 5, 201, 699, 195, 577, 10, 65, 876, 2], 0]\n",
      "[[1, 75, 41, 1051, 699, 2, 892, 9, 614, 356, 29, 2], 1]\n",
      "[[1, 75, 614, 356, 36, 717, 48, 22, 71, 109, 702, 1216, 1531, 1051, 699, 2, 614, 356, 36, 717, 48, 22, 9, 702, 1216, 1216, 1531, 2], 1]\n",
      "[[1, 274, 125, 15, 4, 13, 614, 356, 201, 340, 9, 128, 543, 524, 418, 2, 208, 142, 449, 349, 370, 702, 1216, 1531, 2], 1]\n",
      "[[1, 1051, 699, 46, 49, 10, 955, 48, 22, 160, 87, 138, 192, 201, 10, 297, 48, 22, 29, 61, 1042, 242, 213, 2, 16, 52, 370, 87, 192, 1114, 12045, 2], 1]\n",
      "[[1, 142, 449, 7, 218, 201, 334, 2, 3345, 167, 27, 136, 340, 136, 65, 332, 217, 399, 12045, 160, 152, 201, 699, 5, 543, 2], 0]\n",
      "[[1, 226, 115, 128, 543, 65, 332, 1260, 2, 478, 231, 128, 543, 65, 332, 4, 152, 357, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 777, 9, 581, 824, 958, 4, 255, 201, 699, 603, 1118, 12045, 2, 75, 777, 155, 958, 581, 824, 201, 4, 13, 614, 356, 32, 201, 699, 603, 1118, 12045, 2], 0]\n",
      "[[1, 208, 12049, 335, 16, 45, 702, 1216, 2, 208, 42, 335, 16, 45, 702, 1216, 1531, 275, 374, 2], 0]\n",
      "[[1, 12061, 647, 358, 1010, 7, 125, 201, 699, 48, 22, 2, 530, 125, 22, 49, 201, 936, 314, 2], 1]\n",
      "[[1, 7, 218, 92, 201, 334, 466, 28, 113, 48, 22, 11, 1531, 1114, 12045, 2, 48, 16, 48, 22, 160, 152, 201, 7, 64, 59, 12045, 87, 11, 160, 152, 165, 297, 52, 352, 790, 62, 64, 201, 334, 2], 1]\n",
      "[[1, 226, 115, 36, 717, 416, 75, 1531, 699, 2, 449, 36, 75, 105, 52, 331, 120, 702, 1216, 850, 2], 1]\n",
      "[[1, 75, 1082, 212, 500, 16, 170, 10, 16, 10, 312, 27, 1051, 699, 370, 566, 15, 2, 75, 11, 414, 1046, 707, 52, 1051, 45, 15, 4, 16, 413, 41, 128, 543, 2], 1]\n",
      "[[1, 249, 1051, 699, 603, 1118, 2, 1051, 1531, 938, 647, 603, 1118, 2], 0]\n",
      "[[1, 226, 170, 1082, 1082, 180, 990, 1051, 699, 413, 41, 370, 566, 2, 226, 170, 75, 9, 6927, 9525, 5, 852, 130, 48, 22, 7, 218, 92, 370, 39, 61, 1114, 12045, 2], 1]\n",
      "[[1, 75, 41, 806, 777, 2, 65, 876, 424, 48, 22, 806, 170, 777, 12045, 2], 0]\n",
      "[[1, 647, 427, 75, 5, 693, 212, 852, 130, 743, 13, 177, 211, 12044, 2, 852, 130, 743, 564, 15, 2], 1]\n",
      "[[1, 226, 170, 426, 2008, 7, 125, 201, 32, 936, 314, 2, 75, 1082, 442, 109, 2226, 12044, 695, 159, 181, 125, 45, 2226, 12044, 2556, 195, 7, 125, 51, 725, 212, 9, 347, 639, 1114, 12045, 12045, 12045, 12045, 2], 0]\n",
      "[[1, 614, 356, 36, 717, 53, 558, 328, 2, 449, 36, 41, 323, 75, 175, 29, 2708, 1216, 1531, 2], 1]\n",
      "[[1, 75, 313, 358, 7, 86, 1683, 262, 793, 406, 259, 500, 1086, 10, 936, 314, 1683, 5, 2, 13, 614, 356, 75, 5, 262, 793, 406, 138, 402, 519, 16, 28, 299, 2], 0]\n",
      "[[1, 226, 170, 864, 53, 230, 2, 9, 340, 9, 663, 58, 702, 687, 2], 1]\n",
      "[[1, 75, 41, 201, 699, 406, 181, 2, 88, 215, 334, 406, 181, 2], 1]\n",
      "[[1, 1051, 1531, 195, 577, 2, 195, 577, 2], 0]\n",
      "[[1, 435, 285, 128, 543, 500, 1086, 2, 142, 228, 263, 285, 490, 1006, 247, 98, 500, 1086, 2], 0]\n",
      "[[1, 277, 142, 508, 125, 75, 1051, 15, 1755, 42, 93, 553, 125, 201, 15, 4, 10, 16, 10, 297, 185, 370, 553, 125, 5, 217, 399, 2, 313, 358, 86, 75, 508, 125, 1531, 699, 15, 5736, 7, 27, 604, 195, 49, 62, 64, 201, 381, 299, 12043, 12043, 10, 113, 559, 7, 27, 604, 195, 5, 217, 399, 12043, 12043, 16, 114, 59, 195, 65, 876, 7, 27, 604, 195, 49, 62, 64, 201, 381, 299, 12043, 12043, 2], 0]\n",
      "[[1, 1051, 699, 41, 65, 876, 424, 86, 61, 2, 325, 178, 913, 439, 88, 364, 165, 88, 364, 15, 65, 876, 15, 52, 16, 52, 40, 52, 88, 364, 1114, 113, 32, 178, 2], 1]\n",
      "[[1, 553, 36, 364, 699, 1260, 2, 1531, 553, 125, 45, 2226, 2], 1]\n",
      "[[1, 249, 211, 949, 7, 125, 10, 65, 332, 2, 1294, 167, 27, 136, 238, 399, 65, 332, 12045, 2], 1]\n",
      "[[1, 556, 596, 581, 598, 650, 41, 323, 10, 614, 356, 221, 461, 12045, 2, 13, 614, 356, 75, 597, 496, 154, 72, 1051, 699, 2], 1]\n",
      "[[1, 6, 936, 356, 160, 852, 2, 6, 75, 313, 160, 852, 936, 356, 160, 1260, 2], 1]\n",
      "[[1, 7, 339, 160, 496, 201, 699, 603, 1118, 2, 1227, 500, 201, 699, 3221, 36, 717, 1834, 699, 2], 0]\n",
      "[[1, 936, 356, 129, 1459, 702, 1216, 1531, 369, 52, 12045, 2, 340, 9, 1051, 699, 4, 52, 129, 1459, 702, 1216, 1531, 1114, 12045, 2], 1]\n",
      "[[1, 601, 1051, 699, 49, 170, 876, 52, 185, 45, 128, 543, 524, 418, 2, 13, 614, 356, 1051, 699, 49, 128, 543, 201, 16, 524, 418, 12045, 2], 1]\n",
      "[[1, 269, 872, 218, 179, 28, 577, 2, 903, 211, 4, 48, 22, 1051, 127, 218, 1114, 2], 0]\n",
      "[[1, 702, 1216, 1531, 1256, 309, 478, 246, 48, 22, 938, 647, 2, 16, 10, 295, 1577, 478, 246, 2], 1]\n",
      "[[1, 75, 5, 131, 7, 195, 18017, 18017, 2, 75, 5, 1915, 269, 335, 86, 2], 1]\n",
      "[[1, 1642, 899, 51, 75, 88, 364, 2, 75, 340, 11, 209, 91, 5, 1834, 699, 180, 152, 476, 71, 299, 2], 1]\n",
      "[[1, 75, 48, 22, 7, 218, 92, 201, 334, 1114, 2, 226, 170, 12044, 160, 152, 201, 699, 413, 41, 414, 1046, 217, 399, 356, 12045, 2], 1]\n",
      "[[1, 12, 20, 797, 40, 777, 341, 139, 577, 852, 65, 332, 2, 797, 40, 442, 109, 192, 852, 852, 130, 2], 0]\n",
      "[[1, 217, 475, 17, 73, 10, 65, 332, 2, 702, 1216, 1531, 88, 124, 2], 1]\n",
      "[[1, 702, 539, 777, 5, 442, 39, 852, 130, 13, 65, 332, 2, 11, 205, 195, 442, 39, 45, 797, 40, 777, 4, 341, 125, 165, 577, 108, 852, 130, 1114, 2], 0]\n",
      "[[1, 702, 1216, 1531, 217, 399, 2, 999, 183, 4, 7, 125, 65, 332, 217, 399, 2453, 2], 1]\n",
      "[[1, 13, 614, 356, 204, 1720, 29, 246, 86, 763, 16, 15, 702, 1216, 1531, 2, 3748, 5, 340, 9, 1114, 12045, 2], 1]\n",
      "[[1, 8, 35, 160, 852, 12045, 2, 48, 22, 8, 35, 160, 852, 130, 1114, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 295, 45, 1577, 647, 535, 201, 10, 938, 647, 16, 33, 369, 2, 13, 614, 356, 238, 10, 597, 496, 1173, 132, 318, 220, 990, 1268, 2], 0]\n",
      "[[1, 7, 689, 32, 11, 65, 876, 49, 293, 45, 128, 543, 2, 1051, 699, 598, 592, 36, 143, 2], 1]\n",
      "[[1, 936, 314, 806, 247, 98, 500, 1086, 2, 48, 22, 806, 27, 247, 98, 500, 1086, 598, 592, 1114, 12045, 2], 0]\n",
      "[[1, 247, 98, 3384, 28, 48, 22, 938, 647, 702, 1216, 1531, 1114, 2, 936, 356, 88, 124, 12045, 413, 41, 614, 356, 138, 402, 2], 1]\n",
      "[[1, 442, 1915, 192, 852, 9, 269, 872, 577, 108, 1114, 2, 958, 442, 39, 702, 539, 340, 9, 577, 108, 940, 12045, 2], 1]\n",
      "[[1, 4207, 1051, 167, 125, 65, 332, 958, 2, 508, 125, 1051, 181, 125, 201, 4, 217, 399, 65, 332, 12045, 2], 1]\n",
      "[[1, 7, 339, 597, 496, 556, 596, 581, 702, 539, 797, 40, 598, 650, 41, 323, 2, 936, 356, 52, 596, 581, 702, 539, 4, 797, 40, 598, 650, 41, 323, 2], 0]\n",
      "[[1, 3221, 36, 717, 424, 1577, 647, 75, 1051, 702, 1216, 1531, 2, 449, 36, 32, 1577, 647, 75, 175, 29, 702, 1216, 850, 1091, 2], 0]\n",
      "[[1, 13, 614, 356, 16, 1025, 5, 789, 109, 412, 406, 1086, 15, 2, 13, 614, 356, 7, 339, 118, 441, 789, 109, 412, 406, 1086, 4, 180, 86, 7, 439, 4, 311, 118, 102, 381, 45, 789, 109, 412, 406, 1086, 4, 180, 640, 86, 7, 439, 2], 0]\n",
      "[[1, 88, 124, 702, 2, 3384, 43, 222, 936, 356, 938, 647, 702, 1216, 1531, 2], 0]\n",
      "[[1, 160, 152, 201, 334, 1531, 699, 49, 52, 466, 28, 1531, 131, 177, 218, 1114, 2, 28, 7, 872, 1531, 699, 160, 152, 201, 334, 4, 48, 955, 1051, 1531, 263, 170, 852, 130, 5, 1051, 699, 2], 0]\n",
      "[[1, 28, 1598, 938, 647, 5, 4, 45, 87, 11, 165, 340, 9, 185, 45, 128, 543, 524, 418, 12045, 2, 16, 170, 221, 461, 4, 1082, 424, 618, 88, 15, 4, 47, 32, 464, 48, 22, 293, 818, 128, 543, 2], 1]\n",
      "[[1, 702, 1216, 1531, 109, 270, 13, 614, 356, 32, 16, 373, 15, 2, 250, 61, 9, 4, 87, 11, 340, 15, 2], 1]\n",
      "[[1, 702, 539, 797, 40, 48, 22, 2717, 91, 702, 212, 28, 25, 13, 414, 1046, 2643, 2, 702, 212, 2717, 91, 2], 0]\n",
      "[[1, 160, 152, 201, 699, 29, 63, 44, 797, 40, 777, 16, 347, 639, 940, 2, 52, 158, 348, 5, 797, 40, 777, 155, 201, 699, 356, 2], 1]\n",
      "[[1, 647, 358, 1051, 699, 48, 22, 160, 87, 5, 1114, 2, 105, 113, 10, 178, 48, 22, 160, 87, 4947, 12045, 2], 1]\n",
      "[[1, 215, 334, 269, 28, 1256, 88, 2, 201, 334, 1051, 699, 52, 824, 88, 248, 215, 334, 406, 181, 1114, 12045, 2], 0]\n",
      "[[1, 7, 218, 134, 65, 1051, 397, 211, 51, 1114, 2, 226, 170, 30, 75, 335, 75, 5, 852, 130, 9, 722, 9513, 30, 75, 313, 1051, 722, 9513, 255, 10, 269, 872, 134, 69, 397, 9513, 4, 312, 356, 75, 41, 936, 356, 424, 52, 1051, 45, 722, 9513, 1091, 2], 0]\n",
      "[[1, 397, 125, 556, 293, 45, 128, 543, 2, 614, 356, 36, 717, 32, 445, 61, 524, 418, 128, 543, 2], 0]\n",
      "[[1, 936, 314, 263, 285, 201, 699, 1223, 2028, 777, 12045, 2, 75, 265, 435, 285, 15, 305, 797, 201, 699, 973, 716, 4, 48, 22, 1834, 699, 2], 0]\n",
      "[[1, 142, 228, 75, 806, 247, 98, 936, 356, 315, 12045, 2, 142, 449, 263, 285, 625, 1046, 124, 128, 543, 2], 1]\n",
      "[[1, 614, 356, 36, 143, 48, 22, 45, 2, 13, 449, 958, 201, 16, 45, 1915, 2], 0]\n",
      "[[1, 702, 1216, 1531, 614, 356, 36, 717, 48, 22, 88, 440, 1051, 699, 2, 75, 9, 240, 211, 852, 130, 48, 22, 254, 1051, 7, 211, 1998, 86, 5, 93, 553, 125, 486, 1051, 40, 16, 40, 2], 1]\n",
      "[[1, 226, 170, 12044, 201, 699, 139, 195, 52, 285, 1114, 12045, 2, 226, 170, 4, 52, 16, 52, 285, 7, 86, 201, 699, 139, 12045, 2], 0]\n",
      "[[1, 702, 1216, 1531, 204, 1720, 247, 98, 13, 614, 356, 340, 9, 88, 124, 2, 2502, 228, 247, 98, 52, 88, 124, 1114, 2], 0]\n",
      "[[1, 7, 211, 540, 42, 2802, 12062, 217, 399, 7, 125, 41, 65, 332, 217, 399, 2, 317, 211, 139, 399, 41, 65, 332, 2], 1]\n",
      "[[1, 87, 11, 16, 373, 15, 2, 3633, 6, 340, 9, 15, 2], 0]\n",
      "[[1, 16, 48, 22, 82, 87, 12045, 2, 226, 170, 647, 358, 75, 5, 699, 48, 22, 370, 87, 1114, 12045, 2], 1]\n",
      "[[1, 2207, 183, 59, 284, 27, 136, 217, 399, 65, 332, 2, 159, 559, 217, 399, 2], 1]\n",
      "[[1, 7, 211, 183, 341, 125, 65, 332, 958, 217, 399, 2, 158, 276, 505, 459, 1137, 28, 335, 373, 15, 4, 255, 238, 138, 66, 28, 332, 15, 47, 872, 217, 399, 4, 936, 356, 315, 12045, 2], 1]\n",
      "[[1, 34, 21, 2556, 195, 936, 356, 315, 2, 142, 228, 2556, 195, 192, 852, 10, 2342, 312, 356, 1485, 399, 10, 65, 332, 2], 0]\n",
      "[[1, 9, 340, 9, 1147, 450, 192, 2, 170, 876, 88, 364, 1051, 699, 369, 52, 12045, 12045, 2], 1]\n",
      "[[1, 67, 70, 201, 699, 603, 1118, 2, 43, 65, 84, 36, 143, 12045, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 5, 1531, 699, 938, 647, 171, 1865, 785, 2, 75, 13, 614, 356, 598, 592, 556, 124, 93, 2], 0]\n",
      "[[1, 13, 614, 356, 16, 52, 29, 702, 212, 562, 852, 201, 699, 2, 86, 136, 116, 201, 65, 332, 2], 1]\n",
      "[[1, 59, 195, 49, 160, 152, 201, 699, 4, 217, 399, 936, 356, 185, 370, 2, 142, 228, 59, 195, 49, 160, 152, 201, 334, 1531, 699, 22, 49, 217, 399, 340, 9, 15, 940, 2], 0]\n",
      "[[1, 1294, 7, 27, 136, 65, 332, 958, 2, 852, 130, 4225, 9479, 16, 52, 7, 218, 1051, 39, 61, 1114, 2], 1]\n",
      "[[1, 28, 195, 1915, 269, 2, 75, 65, 2226, 269, 2], 0]\n",
      "[[1, 702, 1216, 1531, 429, 11, 1256, 1091, 12045, 2, 226, 170, 75, 313, 1431, 1137, 7, 86, 12043, 13, 614, 356, 702, 1216, 1531, 75, 1132, 667, 11, 3384, 155, 335, 16, 45, 1091, 2], 0]\n",
      "[[1, 75, 5, 2717, 91, 797, 40, 777, 416, 2263, 141, 98, 943, 15, 2, 936, 314, 2717, 91, 797, 40, 777, 2], 1]\n",
      "[[1, 588, 36, 201, 699, 15, 4, 13, 614, 356, 87, 11, 313, 118, 102, 1051, 958, 16, 48, 22, 1051, 2, 201, 699, 46, 49, 16, 52, 486, 29, 1114, 2], 0]\n",
      "[[1, 317, 211, 139, 399, 41, 65, 332, 2, 226, 170, 4, 3240, 7, 125, 10, 65, 332, 217, 399, 1091, 12045, 1183, 1183, 2], 0]\n",
      "[[1, 797, 40, 445, 128, 543, 93, 61, 340, 293, 4, 19, 1125, 614, 356, 36, 717, 424, 32, 486, 218, 445, 128, 543, 93, 61, 2, 312, 614, 356, 36, 717, 445, 61, 128, 543, 12045, 2], 1]\n",
      "[[1, 88, 16, 15, 246, 2, 647, 358, 75, 936, 356, 88, 16, 15, 702, 1216, 1531, 2], 1]\n",
      "[[1, 226, 170, 4, 75, 313, 806, 128, 543, 500, 4, 427, 46, 152, 312, 27, 500, 806, 1407, 16, 29, 15, 2, 142, 228, 263, 285, 490, 1006, 247, 98, 500, 1086, 2], 1]\n",
      "[[1, 262, 793, 406, 212, 399, 789, 109, 990, 1268, 218, 179, 93, 65, 4, 154, 72, 1051, 958, 2, 75, 11, 175, 29, 36, 196, 262, 793, 406, 500, 1086, 789, 990, 4, 234, 600, 154, 72, 1051, 958, 4, 936, 314, 1666, 243, 93, 61, 2], 0]\n",
      "[[1, 2440, 2440, 4, 951, 226, 836, 75, 88, 124, 86, 4, 178, 15, 312, 356, 65, 1350, 543, 2, 16, 1577, 647, 75, 175, 29, 702, 1216, 1531, 4, 75, 16, 11, 129, 490, 53, 230, 5, 53, 300, 500, 15, 165, 7, 544, 17, 15, 201, 16, 1577, 647, 75, 175, 29, 2], 1]\n",
      "[[1, 226, 170, 4, 20, 403, 136, 136, 2131, 1042, 16, 15, 1260, 2, 544, 989, 13, 614, 356, 16, 520, 20, 403, 161, 645, 2], 1]\n",
      "[[1, 13, 614, 356, 16, 48, 22, 247, 70, 201, 699, 2, 12, 143, 16, 52, 570, 36, 16, 577, 852, 130, 570, 36, 201, 699, 369, 52, 2], 0]\n",
      "[[1, 13, 614, 356, 597, 496, 791, 95, 157, 59, 340, 9, 124, 93, 2, 13, 614, 356, 486, 218, 1051, 699, 598, 592, 16, 124, 93, 2], 0]\n",
      "[[1, 47, 27, 10, 226, 115, 53, 230, 5, 500, 1086, 1114, 2, 508, 125, 46, 103, 48, 22, 445, 128, 543, 1114, 2], 1]\n",
      "[[1, 11, 340, 2, 9, 8, 2643, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 938, 647, 702, 1216, 1531, 355, 160, 496, 178, 262, 793, 212, 399, 789, 109, 990, 1268, 218, 179, 93, 65, 2, 262, 793, 406, 500, 1086, 789, 109, 990, 1268, 4, 160, 496, 154, 72, 938, 647, 4, 310, 936, 356, 239, 38, 12044, 2], 0]\n",
      "[[1, 539, 403, 124, 204, 62, 1114, 2, 226, 170, 74, 13, 614, 356, 883, 704, 16, 28, 299, 2], 1]\n",
      "[[1, 88, 124, 2, 88, 124, 42, 702, 1216, 1531, 2], 0]\n",
      "[[1, 13, 614, 356, 201, 10, 340, 9, 2, 208, 211, 59, 530, 27, 136, 201, 4, 7, 27, 136, 201, 65, 332, 2], 1]\n",
      "[[1, 8025, 5, 217, 399, 10, 65, 332, 2, 217, 399, 10, 588, 125, 201, 10, 12045, 2], 1]\n",
      "[[1, 508, 125, 119, 21, 797, 40, 7, 339, 165, 10, 201, 699, 603, 1118, 936, 356, 201, 1091, 2, 13, 614, 356, 16, 52, 94, 521, 269, 872, 247, 70, 201, 699, 12045, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 340, 9, 702, 1583, 1216, 2, 11, 75, 5, 201, 340, 9, 13, 614, 356, 75, 340, 9, 702, 1216, 140, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 340, 9, 42, 348, 8, 165, 9, 2, 13, 614, 356, 340, 9, 2], 0]\n",
      "[[1, 226, 47, 128, 543, 10, 65, 332, 2, 194, 40, 777, 445, 128, 543, 65, 332, 10, 459, 562, 852, 2], 1]\n",
      "[[1, 990, 93, 15, 293, 128, 543, 936, 356, 315, 2, 634, 93, 903, 59, 959, 556, 185, 45, 128, 543, 2], 0]\n",
      "[[1, 936, 314, 263, 285, 387, 135, 128, 543, 2, 75, 806, 247, 98, 500, 15, 2], 1]\n",
      "[[1, 226, 170, 12044, 201, 699, 139, 195, 52, 285, 1114, 12045, 2, 647, 358, 201, 699, 139, 195, 52, 290, 407, 1114, 12045, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 128, 543, 524, 418, 49, 4, 1051, 699, 603, 1118, 2, 27, 8, 1051, 699, 603, 1118, 250, 196, 2], 1]\n",
      "[[1, 13, 449, 201, 340, 9, 293, 45, 128, 543, 524, 418, 2, 48, 22, 408, 598, 592, 8, 141, 486, 218, 600, 128, 524, 418, 1114, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 938, 647, 178, 61, 128, 543, 4, 45, 87, 11, 201, 340, 61, 128, 2, 226, 115, 165, 340, 9, 445, 128, 543, 1260, 2], 0]\n",
      "[[1, 28, 7, 872, 1531, 699, 160, 152, 201, 334, 4, 48, 955, 1051, 1531, 263, 170, 852, 130, 5, 1051, 699, 2, 160, 152, 201, 334, 212, 29, 48, 160, 69, 2], 0]\n",
      "[[1, 16, 52, 1051, 958, 75, 2069, 2, 7, 689, 10, 201, 334, 553, 872, 852, 130, 424, 52, 1548, 441, 1260, 2], 0]\n",
      "[[1, 160, 152, 201, 334, 32, 9, 217, 399, 356, 2, 160, 152, 201, 328, 49, 4, 65, 84, 36, 143, 48, 22, 486, 218, 1531, 2], 0]\n",
      "[[1, 13, 614, 356, 201, 699, 49, 16, 52, 11, 1051, 15, 12045, 2, 62, 64, 18017, 699, 75, 5, 16659, 130, 18017, 18017, 9, 18017, 2], 0]\n",
      "[[1, 2189, 8, 4, 1256, 155, 9, 1051, 958, 1260, 2, 1051, 699, 11, 1256, 2], 1]\n",
      "[[1, 142, 449, 349, 116, 702, 1216, 1531, 138, 2, 75, 41, 88, 124, 13, 614, 356, 75, 16, 52, 88, 124, 2], 1]\n",
      "[[1, 142, 449, 263, 285, 695, 721, 247, 98, 500, 1086, 2, 22, 152, 247, 98, 500, 806, 15, 936, 356, 315, 2], 0]\n",
      "[[1, 75, 936, 356, 129, 1459, 47, 27, 369, 52, 1082, 16, 96, 111, 88, 15, 2, 936, 356, 370, 566, 369, 52, 2], 1]\n",
      "[[1, 797, 40, 445, 128, 543, 93, 61, 340, 293, 4, 19, 1125, 614, 356, 36, 717, 424, 32, 486, 218, 445, 128, 543, 93, 61, 2, 75, 313, 358, 614, 356, 36, 717, 32, 486, 445, 128, 543, 93, 61, 1091, 12045, 2], 1]\n",
      "[[1, 22, 938, 647, 1051, 699, 852, 130, 4, 65, 332, 36, 717, 34, 364, 2, 265, 34, 39, 938, 647, 449, 36, 45, 2], 0]\n",
      "[[1, 204, 62, 1114, 12047, 62, 497, 387, 305, 2, 1256, 27, 161, 645, 53, 230, 414, 1046, 204, 62, 1022, 161, 843, 2], 1]\n",
      "[[1, 226, 170, 12044, 508, 86, 1598, 2992, 340, 9, 1834, 75, 699, 12045, 2, 768, 180, 340, 1834, 958, 2], 0]\n",
      "[[1, 75, 936, 356, 11, 702, 212, 828, 16, 45, 702, 1216, 1531, 12045, 2, 892, 170, 4, 142, 228, 3384, 171, 1860, 4, 83, 571, 702, 539, 797, 40, 500, 54, 3384, 500, 2717, 91, 4, 1915, 246, 32, 16, 32, 476, 11, 260, 645, 12045, 2], 1]\n",
      "[[1, 10, 16, 10, 1531, 699, 413, 41, 247, 607, 453, 5, 2, 75, 313, 358, 7, 86, 702, 1216, 1531, 211, 59, 46, 346, 5, 217, 399, 165, 10, 341, 136, 136, 651, 7, 200, 276, 1114, 12045, 2], 0]\n",
      "[[1, 340, 9, 8, 416, 75, 445, 128, 543, 524, 418, 1260, 2, 340, 9, 8, 35, 1057, 75, 387, 135, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 5, 62, 64, 201, 328, 46, 49, 113, 340, 9, 852, 130, 2340, 12045, 2, 13, 614, 356, 1577, 647, 75, 42, 340, 852, 130, 2], 1]\n",
      "[[1, 1051, 342, 212, 399, 2, 1915, 269, 139, 2], 1]\n",
      "[[1, 131, 7, 218, 1834, 699, 603, 1118, 4, 449, 36, 486, 1834, 12045, 2, 647, 358, 1051, 699, 217, 399, 158, 614, 356, 36, 717, 88, 440, 559, 2], 1]\n",
      "[[1, 142, 228, 370, 566, 702, 217, 1531, 2, 142, 449, 129, 1459, 702, 1216, 1531, 852, 130, 2], 0]\n",
      "[[1, 16, 41, 1051, 958, 2, 75, 87, 11, 41, 370, 566, 693, 438, 2], 1]\n",
      "[[1, 797, 40, 695, 721, 500, 1086, 263, 285, 15, 185, 16, 45, 412, 406, 1086, 2, 412, 406, 1086, 34, 45, 1025, 29, 5, 247, 98, 500, 1086, 28, 936, 356, 315, 2], 0]\n",
      "[[1, 13, 614, 356, 938, 647, 22, 49, 797, 40, 7, 339, 340, 387, 135, 1091, 2, 177, 167, 274, 96, 36, 165, 52, 598, 592, 1114, 12043, 12043, 2], 1]\n",
      "[[1, 208, 340, 9, 429, 275, 2, 158, 1256, 1531, 2], 1]\n",
      "[[1, 7, 689, 791, 95, 480, 1349, 41, 65, 876, 2, 7, 689, 32, 1388, 65, 84, 36, 143, 486, 445, 1260, 2], 0]\n",
      "[[1, 75, 5, 702, 1216, 1531, 697, 187, 16, 373, 15, 2, 75, 87, 11, 48, 335, 45, 702, 217, 1531, 429, 275, 4, 255, 180, 640, 49, 597, 496, 280, 76, 93, 195, 4, 154, 72, 597, 496, 12046, 312, 75, 936, 356, 201, 699, 2453, 12045, 2], 1]\n",
      "[[1, 1045, 285, 632, 1086, 2, 435, 285, 632, 1086, 2], 0]\n",
      "[[1, 48, 22, 160, 152, 7, 218, 201, 699, 1114, 2, 1051, 699, 49, 48, 22, 7, 218, 160, 152, 201, 334, 110, 9, 1051, 699, 1114, 2], 0]\n",
      "[[1, 48, 22, 435, 285, 2717, 91, 797, 40, 777, 1114, 2, 10, 16, 10, 2717, 91, 797, 40, 777, 113, 48, 22, 1531, 699, 15, 12045, 2], 1]\n",
      "[[1, 355, 10, 178, 75, 262, 793, 406, 789, 109, 218, 179, 990, 1268, 93, 65, 2, 262, 793, 406, 212, 399, 789, 109, 990, 1268, 218, 179, 93, 65, 4, 154, 72, 1051, 958, 2], 0]\n",
      "[[1, 614, 356, 36, 717, 45, 1915, 2, 614, 356, 36, 717, 52, 1531, 699, 265, 60, 129, 490, 15, 208, 27, 136, 15, 2], 1]\n",
      "[[1, 938, 647, 22, 49, 413, 41, 65, 84, 36, 143, 424, 9, 566, 399, 2, 1051, 699, 201, 11, 938, 647, 12, 4, 263, 285, 15, 695, 721, 247, 98, 500, 2], 1]\n",
      "[[1, 7, 689, 1051, 699, 10, 614, 356, 36, 717, 45, 2, 13, 614, 356, 2876, 125, 201, 52, 1531, 4, 508, 125, 535, 178, 598, 592, 16, 124, 93, 2], 0]\n",
      "[[1, 226, 41, 323, 5, 412, 406, 414, 1046, 632, 1086, 10, 614, 356, 242, 242, 2, 435, 285, 632, 1086, 2], 1]\n",
      "[[1, 936, 356, 263, 806, 201, 699, 777, 500, 12045, 2, 936, 356, 263, 806, 201, 699, 797, 40, 2], 0]\n",
      "[[1, 556, 596, 581, 53, 539, 797, 40, 598, 650, 2, 44, 178, 556, 596, 581, 702, 539, 797, 40, 598, 650, 41, 323, 4, 154, 72, 1051, 1531, 699, 12045, 2], 1]\n",
      "[[1, 336, 1377, 1572, 299, 1256, 350, 2, 1051, 699, 336, 1377, 1572, 2], 1]\n",
      "[[1, 48, 22, 160, 152, 7, 218, 201, 699, 1114, 2, 75, 313, 358, 7, 86, 4, 160, 152, 201, 699, 5, 543, 4, 113, 16, 52, 1051, 699, 15, 12045, 2], 0]\n",
      "[[1, 240, 218, 201, 699, 165, 603, 1118, 15, 4, 9, 63, 44, 201, 699, 58, 220, 2, 508, 125, 52, 16, 52, 2008, 309, 201, 699, 4, 75, 41, 299, 797, 40, 442, 1915, 2], 0]\n",
      "[[1, 11, 1256, 155, 350, 336, 1377, 907, 2, 142, 449, 349, 370, 783, 399, 1572, 2], 0]\n",
      "[[1, 16, 10, 1577, 647, 5, 478, 8, 113, 16, 52, 1051, 699, 1114, 2, 16, 10, 1577, 647, 5, 478, 246, 48, 22, 1051, 958, 1114, 2], 0]\n",
      "[[1, 263, 102, 15, 102, 5, 3887, 105, 10, 16, 52, 883, 704, 12043, 10, 16, 10, 226, 115, 508, 125, 135, 214, 9, 358, 281, 12043, 2, 936, 356, 87, 11, 16, 52, 883, 891, 12045, 2], 1]\n",
      "[[1, 75, 313, 358, 7, 86, 1683, 262, 793, 406, 259, 500, 1086, 10, 936, 314, 1683, 5, 2, 13, 614, 356, 75, 789, 109, 5, 262, 793, 406, 500, 10, 990, 1268, 5, 4, 75, 10, 588, 575, 262, 793, 406, 28, 61, 789, 5, 2], 1]\n",
      "[[1, 75, 5, 201, 699, 139, 195, 2, 201, 699, 139, 195, 2], 0]\n",
      "[[1, 614, 356, 36, 717, 48, 22, 1051, 699, 2453, 2, 614, 356, 36, 717, 48, 22, 486, 218, 175, 29, 702, 1216, 1531, 2], 0]\n",
      "[[1, 75, 4376, 340, 702, 1216, 1531, 2, 75, 5, 247, 98, 4, 340, 75, 702, 1216, 1531, 4, 936, 356, 381, 104, 12045, 2], 0]\n",
      "[[1, 445, 128, 543, 340, 293, 45, 936, 356, 315, 2, 1082, 424, 128, 543, 340, 9, 293, 45, 2], 0]\n",
      "[[1, 4207, 1051, 167, 125, 65, 332, 958, 2, 508, 125, 1051, 7, 608, 181, 125, 201, 65, 332, 2], 1]\n",
      "[[1, 783, 399, 1572, 2, 589, 125, 783, 399, 205, 70, 567, 766, 153, 125, 175, 29, 356, 2], 0]\n",
      "[[1, 403, 933, 777, 138, 192, 903, 211, 4, 142, 228, 181, 125, 313, 1042, 109, 20, 403, 8, 1422, 515, 12062, 5, 38, 625, 66, 100, 4, 124, 93, 702, 539, 797, 40, 3887, 442, 109, 138, 192, 9, 577, 852, 1114, 12045, 2, 66, 100, 3269, 381, 10, 955, 413, 41, 247, 607, 453, 12045, 2], 1]\n",
      "[[1, 226, 170, 87, 11, 48, 22, 445, 128, 543, 2340, 2, 48, 22, 445, 128, 543, 416, 75, 15, 601, 2], 1]\n",
      "[[1, 647, 358, 48, 22, 29, 348, 5, 797, 40, 777, 201, 958, 1114, 2, 142, 449, 352, 790, 63, 44, 797, 40, 777, 247, 70, 201, 699, 2], 1]\n",
      "[[1, 52, 29, 1201, 958, 201, 699, 356, 2, 48, 22, 11, 1201, 958, 155, 1834, 699, 1114, 2], 0]\n",
      "[[1, 442, 39, 577, 852, 65, 332, 12045, 2, 702, 539, 38, 625, 51, 447, 138, 123, 17, 1417, 9, 154, 577, 108, 12045, 2], 0]\n",
      "[[1, 7, 689, 160, 276, 49, 65, 876, 32, 61, 128, 543, 524, 418, 12045, 2, 7, 689, 797, 40, 41, 45, 614, 356, 36, 143, 424, 32, 445, 128, 543, 93, 61, 524, 418, 1260, 12045, 75, 675, 111, 43, 32, 32, 1695, 184, 15, 2], 1]\n",
      "[[1, 13, 614, 356, 355, 10, 160, 496, 154, 72, 459, 335, 852, 130, 2, 13, 614, 356, 9, 429, 275, 340, 9, 852, 130, 2], 0]\n",
      "[[1, 601, 4, 75, 508, 125, 201, 699, 603, 1118, 2643, 2, 777, 155, 9, 958, 4, 201, 699, 603, 1118, 4, 1082, 424, 29, 702, 212, 201, 699, 4, 201, 160, 1519, 201, 699, 603, 1118, 4, 47, 10, 13, 614, 356, 12045, 2], 1]\n",
      "[[1, 702, 1216, 1531, 75, 936, 340, 2453, 2, 702, 1216, 1531, 936, 356, 340, 88, 364, 12045, 2], 0]\n",
      "[[1, 13, 614, 356, 2795, 340, 9, 2, 226, 170, 1082, 1082, 75, 247, 98, 340, 128, 15, 2], 0]\n",
      "[[1, 142, 449, 263, 285, 695, 721, 247, 98, 500, 2, 263, 285, 695, 721, 247, 98, 500, 1086, 2], 0]\n",
      "[[1, 317, 211, 139, 399, 41, 65, 332, 2, 346, 608, 65, 332, 399, 2], 0]\n",
      "[[1, 194, 40, 2717, 91, 52, 315, 1114, 2, 226, 170, 4, 75, 11, 702, 1216, 1531, 1051, 5, 958, 2717, 91, 201, 699, 5, 797, 40, 777, 566, 1512, 15, 4, 75, 299, 797, 40, 806, 15, 501, 102, 5, 777, 4, 86, 218, 201, 699, 2717, 91, 5, 777, 340, 29, 15, 936, 356, 315, 12045, 2], 1]\n",
      "[[1, 226, 170, 1051, 201, 699, 374, 704, 48, 22, 2654, 599, 5, 1114, 2, 142, 228, 2654, 599, 201, 699, 374, 704, 2], 1]\n",
      "[[1, 938, 647, 553, 36, 52, 315, 86, 61, 2, 65, 876, 263, 102, 88, 124, 132, 2], 1]\n",
      "[[1, 312, 13, 614, 356, 75, 5, 247, 98, 335, 16, 45, 1091, 12045, 2, 75, 5, 247, 98, 28, 16, 597, 496, 1051, 958, 177, 436, 2], 1]\n",
      "[[1, 936, 356, 418, 406, 262, 793, 406, 259, 2, 75, 4376, 356, 388, 79, 132, 418, 406, 2], 1]\n",
      "[[1, 381, 128, 4, 1082, 424, 247, 98, 16, 11, 2, 7, 91, 41, 29, 170, 5, 247, 98, 424, 48, 22, 2643, 2], 1]\n",
      "[[1, 88, 124, 2, 12061, 936, 356, 88, 124, 2], 0]\n",
      "[[1, 702, 1216, 850, 769, 18017, 2, 16, 48, 22, 1531, 12045, 2], 1]\n",
      "[[1, 262, 793, 4376, 412, 406, 2, 142, 449, 262, 793, 412, 406, 2], 0]\n",
      "[[1, 702, 1216, 850, 769, 965, 2, 251, 88, 702, 1216, 1531, 48, 22, 16, 2], 1]\n",
      "[[1, 355, 10, 178, 75, 262, 793, 406, 789, 109, 218, 179, 990, 1268, 93, 65, 2, 75, 131, 7, 218, 870, 4, 75, 519, 132, 436, 14, 262, 793, 406, 500, 1086, 380, 178, 75, 262, 793, 406, 500, 1086, 990, 1268, 65, 10, 614, 356, 221, 461, 2], 0]\n",
      "[[1, 1051, 270, 65, 876, 45, 1915, 2, 1051, 328, 699, 958, 2008, 2008, 16, 45, 1915, 2], 1]\n",
      "[[1, 88, 124, 7, 86, 75, 5, 940, 2, 88, 124, 1260, 4, 19, 1093, 2], 1]\n",
      "[[1, 936, 356, 290, 407, 852, 130, 12045, 2, 290, 852, 2], 1]\n",
      "[[1, 75, 702, 1216, 1531, 16, 373, 15, 4, 129, 98, 49, 201, 10, 16, 40, 2, 109, 270, 566, 603, 15, 2], 1]\n",
      "[[1, 445, 128, 543, 340, 293, 45, 936, 356, 315, 2, 340, 9, 293, 45, 128, 543, 2], 1]\n",
      "[[1, 142, 449, 263, 285, 695, 721, 128, 543, 2, 806, 27, 247, 98, 2], 0]\n",
      "[[1, 226, 170, 12044, 9, 340, 9, 478, 231, 128, 543, 500, 1086, 12045, 2, 1477, 96, 36, 48, 22, 1931, 445, 128, 543, 1114, 12045, 2], 1]\n",
      "[[1, 1051, 958, 2, 142, 449, 459, 335, 67, 324, 201, 699, 182, 617, 2], 0]\n",
      "[[1, 75, 131, 7, 27, 128, 543, 340, 293, 45, 4, 614, 356, 36, 717, 32, 61, 131, 177, 27, 128, 543, 12045, 2, 1082, 424, 340, 293, 45, 128, 543, 2], 1]\n",
      "[[1, 160, 276, 46, 49, 614, 356, 36, 717, 598, 592, 2, 413, 41, 128, 543, 524, 418, 4, 65, 876, 424, 32, 445, 128, 543, 2], 1]\n",
      "[[1, 508, 125, 748, 28, 5, 201, 699, 340, 9, 33, 369, 2, 508, 139, 201, 699, 556, 33, 369, 2], 0]\n",
      "[[1, 75, 614, 356, 36, 717, 52, 938, 647, 2, 16, 416, 88, 124, 2], 1]\n",
      "[[1, 7, 689, 10, 65, 332, 180, 67, 70, 1834, 699, 2, 7, 689, 614, 356, 36, 717, 1834, 699, 2], 0]\n",
      "[[1, 1082, 424, 1879, 293, 128, 543, 87, 11, 340, 445, 61, 93, 2, 990, 93, 598, 592, 128, 543, 2], 0]\n",
      "[[1, 16, 52, 883, 891, 936, 356, 315, 2, 13, 449, 157, 357, 883, 704, 16, 15, 12045, 2], 1]\n",
      "[[1, 75, 48, 22, 29, 702, 212, 1201, 958, 201, 1114, 12045, 2, 201, 699, 1834, 777, 201, 10, 1834, 1201, 958, 2], 1]\n",
      "[[1, 13, 614, 356, 348, 8, 247, 98, 9, 2, 7, 91, 41, 826, 52, 247, 98, 1114, 12045, 2502, 228, 247, 98, 16, 40, 1114, 12045, 2], 0]\n",
      "[[1, 39, 87, 154, 72, 1051, 958, 10, 3221, 182, 617, 2, 28, 76, 597, 496, 75, 16, 52, 1051, 958, 4, 614, 356, 250, 196, 1091, 12045, 2], 1]\n",
      "[[1, 508, 125, 1051, 181, 125, 201, 4, 217, 399, 65, 332, 12045, 2, 3803, 9479, 1051, 722, 125, 217, 399, 65, 332, 2], 1]\n",
      "[[1, 142, 228, 7, 218, 92, 201, 334, 10, 16, 10, 16, 41, 217, 399, 2, 142, 228, 1051, 958, 7, 27, 136, 201, 334, 9, 1147, 450, 192, 1114, 12045, 2], 0]\n",
      "[[1, 892, 170, 75, 313, 129, 1459, 702, 1216, 1531, 47, 27, 369, 52, 2, 936, 356, 370, 566, 369, 52, 2], 1]\n",
      "[[1, 370, 566, 75, 5, 702, 1216, 1531, 2, 142, 228, 370, 566, 702, 217, 1531, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 828, 45, 702, 1216, 1531, 109, 270, 2, 13, 449, 75, 340, 702, 1216, 1531, 2], 1]\n",
      "[[1, 13, 614, 356, 177, 167, 59, 959, 15, 201, 340, 185, 45, 699, 2, 16, 10, 178, 1051, 699, 903, 59, 959, 113, 45, 1915, 1114, 12045, 936, 356, 87, 11, 201, 340, 9, 381, 472, 1091, 12044, 2], 1]\n",
      "[[1, 142, 449, 424, 52, 435, 285, 201, 699, 139, 2, 142, 449, 938, 647, 435, 285, 201, 699, 139, 2], 0]\n",
      "[[1, 938, 647, 15, 7, 689, 65, 876, 32, 445, 128, 543, 93, 61, 2, 3384, 28, 702, 1216, 1531, 4, 65, 876, 32, 445, 128, 543, 524, 418, 2], 1]\n",
      "[[1, 936, 356, 315, 12045, 2, 601, 4, 12, 468, 5, 41, 936, 314, 721, 31, 1392, 1091, 12045, 2], 1]\n",
      "[[1, 903, 59, 959, 48, 22, 45, 1915, 10, 1114, 2, 903, 59, 959, 46, 103, 340, 185, 45, 524, 418, 128, 543, 10, 614, 356, 250, 196, 2], 1]\n",
      "[[1, 263, 806, 247, 98, 500, 1114, 2, 41, 282, 263, 128, 543, 500, 1086, 936, 356, 263, 285, 2], 0]\n",
      "[[1, 16, 10, 1577, 647, 5, 478, 8, 52, 1531, 1114, 2, 16, 10, 702, 1216, 1531, 1577, 647, 5, 478, 8, 2], 1]\n",
      "[[1, 4225, 65, 332, 217, 399, 2, 217, 399, 10, 588, 211, 59, 46, 346, 159, 559, 1114, 12045, 2], 0]\n",
      "[[1, 34, 27, 783, 399, 1572, 61, 2, 783, 399, 336, 1377, 9332, 936, 356, 350, 2], 1]\n",
      "[[1, 836, 75, 129, 1407, 702, 1216, 1531, 2, 702, 1216, 1531, 48, 22, 158, 702, 212, 28, 370, 566, 1407, 1114, 2], 1]\n",
      "[[1, 75, 160, 152, 201, 699, 15, 614, 356, 52, 486, 1051, 1091, 2, 75, 160, 152, 201, 699, 15, 10, 16, 10, 311, 48, 22, 1051, 2], 1]\n",
      "[[1, 836, 75, 427, 852, 130, 743, 45, 2207, 48, 22, 1114, 12045, 2, 226, 170, 4, 852, 130, 48, 22, 820, 332, 1114, 2], 1]\n",
      "[[1, 938, 647, 1051, 699, 49, 65, 876, 52, 293, 45, 128, 543, 2, 75, 313, 358, 7, 86, 4, 47, 27, 1051, 699, 32, 202, 466, 45, 1915, 1114, 12045, 2], 0]\n",
      "[[1, 647, 358, 4, 13, 614, 356, 75, 88, 16, 15, 246, 1091, 12047, 2, 936, 356, 88, 16, 15, 246, 12045, 340, 9, 71, 109, 278, 29, 1114, 12045, 2], 1]\n",
      "[[1, 13, 614, 356, 7, 339, 293, 16, 45, 797, 40, 128, 543, 524, 418, 2, 614, 356, 36, 717, 598, 650, 9, 215, 228, 2], 1]\n",
      "[[1, 1051, 699, 797, 40, 777, 2, 7364, 10043, 9606, 10022, 9899, 9672, 9994, 9961, 10140, 4, 194, 40, 2], 0]\n",
      "[[1, 152, 703, 125, 783, 399, 10, 384, 5, 1114, 2, 589, 125, 783, 399, 205, 70, 567, 766, 153, 125, 175, 29, 356, 2], 0]\n",
      "[[1, 702, 1216, 1531, 304, 259, 2, 936, 356, 459, 1137, 702, 1216, 1531, 852, 130, 2], 0]\n",
      "[[1, 91, 195, 476, 699, 217, 475, 2, 864, 40, 91, 195, 476, 699, 16, 10, 140, 38, 63, 380, 98, 307, 5, 940, 2], 0]\n",
      "[[1, 75, 13, 614, 356, 75, 10, 295, 1577, 478, 246, 45, 340, 9, 852, 130, 12045, 2, 88, 124, 2], 1]\n",
      "[[1, 7, 689, 201, 699, 195, 65, 876, 5, 2, 1051, 699, 49, 65, 84, 36, 143, 201, 699, 2], 0]\n",
      "[[1, 142, 228, 75, 806, 247, 98, 936, 356, 315, 12045, 2, 806, 27, 247, 98, 500, 940, 12043, 2], 1]\n",
      "[[1, 16, 264, 151, 201, 699, 36, 143, 2, 2876, 125, 1051, 5, 643, 608, 508, 125, 313, 201, 627, 608, 2], 0]\n",
      "[[1, 1051, 328, 699, 958, 2008, 2008, 16, 45, 1915, 2, 1051, 958, 43, 27, 128, 543, 15, 2], 1]\n",
      "[[1, 217, 399, 10, 588, 125, 159, 559, 201, 10, 588, 136, 159, 559, 12045, 2, 142, 1051, 699, 3345, 12046, 59, 530, 195, 12044, 131, 7, 136, 201, 699, 6178, 12044, 47, 2169, 10, 153, 136, 5, 217, 399, 201, 10, 770, 137, 215, 559, 12044, 2], 1]\n",
      "[[1, 7, 689, 160, 276, 49, 65, 876, 32, 61, 128, 543, 524, 418, 12045, 2, 1051, 699, 598, 592, 413, 41, 65, 84, 36, 143, 2], 1]\n",
      "[[1, 13, 614, 356, 772, 27, 96, 36, 15, 201, 340, 9, 787, 212, 12045, 2, 75, 340, 9, 1051, 699, 4, 13, 614, 356, 416, 75, 34, 47, 27, 212, 399, 2], 1]\n",
      "[[1, 75, 5, 3384, 13, 614, 356, 340, 9, 702, 1216, 1531, 1091, 2, 75, 3384, 340, 9, 702, 1216, 1531, 12043, 75, 41, 936, 356, 1918, 2], 1]\n",
      "[[1, 681, 500, 1201, 1947, 340, 9, 1834, 699, 2453, 4, 2226, 28, 9, 958, 2453, 2, 59, 195, 201, 699, 49, 852, 130, 340, 9, 282, 73, 2], 1]\n",
      "[[1, 47, 27, 10, 614, 356, 53, 230, 2, 226, 115, 269, 144, 41, 8, 1114, 2], 1]\n",
      "[[1, 702, 1216, 1531, 614, 356, 36, 717, 48, 22, 88, 440, 1051, 699, 2, 113, 10, 553, 180, 88, 440, 48, 22, 1531, 699, 2], 1]\n",
      "[[1, 12061, 335, 16, 45, 702, 1216, 1531, 429, 275, 2, 249, 96, 36, 15, 13, 449, 201, 16, 364, 699, 12045, 311, 16, 52, 1785, 2], 1]\n",
      "[[1, 201, 699, 777, 500, 282, 263, 4, 142, 449, 868, 25, 12043, 2, 201, 699, 777, 265, 60, 282, 263, 4, 142, 449, 273, 2717, 12045, 2], 0]\n",
      "[[1, 936, 356, 129, 1459, 2, 702, 212, 490, 579, 15, 486, 1087, 29, 702, 212, 702, 217, 1531, 113, 16, 373, 15, 2], 1]\n",
      "[[1, 142, 449, 129, 1459, 702, 1216, 1531, 852, 130, 2, 370, 566, 118, 1531, 2], 1]\n",
      "[[1, 3384, 28, 105, 597, 496, 1051, 699, 603, 1118, 2, 75, 16, 264, 151, 13, 614, 356, 1051, 699, 16, 33, 369, 2], 0]\n",
      "[[1, 16, 96, 111, 2556, 195, 7, 125, 936, 356, 201, 699, 2, 226, 170, 426, 2008, 7, 125, 201, 32, 936, 314, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 5, 938, 647, 340, 124, 93, 2, 13, 596, 581, 797, 40, 598, 650, 41, 323, 2], 1]\n",
      "[[1, 86, 763, 204, 371, 93, 49, 4, 702, 212, 28, 76, 936, 356, 340, 9, 597, 496, 12045, 2, 86, 763, 170, 15, 4, 201, 10, 340, 9, 816, 4, 2], 0]\n",
      "[[1, 647, 358, 10, 955, 48, 22, 160, 87, 12045, 2, 958, 48, 22, 160, 87, 1114, 2], 1]\n",
      "[[1, 2502, 228, 247, 98, 5, 105, 10, 7, 314, 5, 1114, 12045, 2, 2502, 228, 247, 98, 936, 356, 315, 12045, 2], 0]\n",
      "[[1, 13, 614, 356, 178, 75, 262, 793, 789, 109, 990, 1268, 154, 72, 1051, 958, 2, 702, 1216, 1531, 603, 1118, 250, 196, 2], 1]\n",
      "[[1, 836, 75, 743, 564, 852, 130, 2, 290, 743, 852, 130, 2], 0]\n",
      "[[1, 13, 614, 356, 598, 650, 16, 93, 2, 13, 614, 356, 598, 592, 16, 93, 1091, 12044, 226, 115, 47, 10, 2189, 96, 751, 5, 940, 2], 0]\n",
      "[[1, 13, 614, 356, 480, 1349, 556, 124, 93, 2, 13, 614, 356, 791, 95, 480, 1349, 340, 124, 93, 4, 614, 356, 221, 461, 12045, 2], 0]\n",
      "[[1, 449, 36, 32, 647, 75, 175, 29, 2, 614, 356, 36, 717, 424, 52, 315, 75, 41, 702, 1216, 1531, 2], 1]\n",
      "[[1, 217, 399, 4376, 356, 559, 2, 217, 399, 7, 125, 559, 65, 332, 2], 0]\n",
      "[[1, 601, 4, 13, 614, 356, 3269, 381, 5, 340, 45, 1915, 2, 13, 614, 356, 75, 1825, 536, 392, 201, 340, 45, 1915, 1091, 2], 0]\n",
      "[[1, 13, 614, 356, 702, 1216, 1531, 178, 75, 480, 1349, 16, 124, 93, 2, 13, 614, 356, 75, 3384, 9, 702, 1216, 1531, 535, 938, 647, 16, 15, 2], 0]\n",
      "[[1, 598, 592, 128, 543, 614, 356, 36, 717, 445, 2, 598, 592, 65, 876, 424, 32, 445, 128, 543, 2], 0]\n",
      "[[1, 142, 449, 418, 406, 2, 13, 614, 356, 75, 79, 132, 418, 406, 285, 606, 16, 15, 2], 0]\n",
      "[[1, 208, 1051, 699, 34, 364, 603, 1118, 2, 75, 16, 264, 151, 13, 614, 356, 1051, 699, 16, 33, 369, 2], 0]\n",
      "[[1, 13, 614, 356, 408, 75, 789, 109, 15, 1051, 958, 412, 406, 1086, 14, 414, 1046, 632, 1086, 311, 381, 45, 250, 61, 280, 76, 2, 16, 40, 2453, 355, 10, 118, 441, 41, 75, 1683, 519, 414, 1046, 632, 1086, 14, 247, 98, 412, 406, 1086, 2], 0]\n",
      "[[1, 13, 124, 93, 791, 95, 480, 1349, 2, 13, 449, 480, 1349, 556, 52, 124, 93, 2], 0]\n",
      "[[1, 88, 16, 15, 246, 2, 13, 614, 88, 246, 16, 33, 369, 2], 0]\n",
      "[[1, 226, 170, 75, 936, 314, 263, 806, 201, 699, 797, 40, 777, 2, 806, 777, 2], 1]\n",
      "[[1, 226, 170, 48, 22, 29, 702, 212, 1201, 958, 201, 699, 356, 2, 226, 170, 4, 958, 11, 702, 212, 155, 76, 105, 48, 22, 201, 702, 1216, 1531, 1114, 12045, 2], 0]\n",
      "[[1, 7, 339, 340, 9, 45, 1915, 2, 508, 125, 75, 442, 39, 45, 81, 40, 208, 211, 183, 4, 340, 9, 45, 1915, 1260, 2], 0]\n",
      "[[1, 75, 313, 160, 152, 201, 699, 48, 22, 1114, 2, 936, 356, 315, 2], 1]\n",
      "[[1, 13, 614, 356, 154, 72, 598, 459, 124, 93, 2, 13, 614, 356, 75, 5, 1051, 699, 11, 598, 592, 2], 0]\n",
      "[[1, 75, 5, 702, 212, 936, 356, 340, 9, 1051, 958, 5, 160, 496, 1091, 2, 341, 27, 136, 9, 787, 212, 160, 1519, 1114, 2], 0]\n",
      "[[1, 48, 22, 160, 152, 7, 218, 201, 699, 1114, 2, 142, 228, 160, 152, 201, 699, 49, 76, 5, 217, 399, 201, 185, 1114, 12045, 2], 1]\n",
      "[[1, 75, 47, 554, 282, 263, 797, 40, 777, 936, 356, 201, 699, 2, 936, 356, 285, 201, 699, 797, 40, 777, 2], 0]\n",
      "[[1, 1755, 136, 217, 399, 10, 65, 332, 2, 226, 47, 217, 399, 10, 16, 10, 3345, 183, 341, 136, 2169, 183, 217, 399, 2], 0]\n",
      "[[1, 702, 1216, 1531, 1051, 958, 16, 373, 15, 2, 13, 614, 356, 75, 702, 1216, 1531, 1531, 16, 15, 1091, 12045, 2], 1]\n",
      "[[1, 18017, 124, 2, 226, 836, 75, 88, 124, 1051, 699, 2], 1]\n",
      "[[1, 7, 125, 217, 399, 170, 65, 958, 2, 1755, 183, 7, 27, 136, 201, 334, 41, 1834, 65, 332, 217, 399, 12045, 2], 1]\n",
      "[[1, 936, 356, 160, 152, 201, 334, 12045, 2, 142, 449, 160, 152, 201, 699, 2], 1]\n",
      "[[1, 194, 40, 442, 109, 852, 130, 2, 66, 100, 3269, 381, 10, 955, 413, 41, 247, 607, 453, 12045, 2], 1]\n",
      "[[1, 204, 1720, 511, 2, 204, 1720, 511, 340, 9, 702, 1216, 1531, 2], 0]\n",
      "[[1, 7, 689, 201, 699, 195, 65, 876, 5, 2, 226, 115, 47, 27, 1531, 699, 195, 577, 10, 65, 876, 2], 0]\n",
      "[[1, 852, 130, 10, 65, 332, 12045, 2, 647, 358, 4, 75, 5, 1531, 699, 852, 130, 10, 65, 332, 12045, 2], 0]\n",
      "[[1, 3384, 155, 76, 340, 9, 702, 1216, 256, 936, 356, 315, 2, 3384, 958, 392, 155, 340, 828, 45, 702, 1216, 1531, 2], 0]\n",
      "[[1, 722, 27, 136, 208, 211, 183, 139, 399, 10, 65, 332, 2, 75, 702, 1531, 15, 7, 211, 274, 4, 647, 358, 139, 399, 10, 65, 332, 12045, 2], 1]\n",
      "[[1, 48, 22, 416, 75, 88, 27, 215, 334, 406, 181, 1114, 2, 75, 46, 152, 11, 702, 1216, 1531, 1051, 15, 317, 211, 183, 4, 28, 27, 136, 265, 201, 334, 4, 13, 614, 356, 75, 5, 725, 212, 28, 597, 496, 556, 215, 334, 12045, 12045, 2], 1]\n",
      "[[1, 75, 265, 88, 246, 2, 129, 1459, 88, 246, 2], 0]\n",
      "[[1, 75, 265, 60, 33, 369, 1531, 93, 240, 218, 699, 12043, 105, 165, 107, 36, 201, 699, 15, 12043, 87, 11, 105, 9, 852, 130, 18008, 48, 10, 160, 1519, 75, 16, 52, 1531, 699, 12043, 47, 10, 936, 356, 381, 104, 464, 2, 75, 201, 699, 15, 13, 614, 356, 16, 48, 22, 1051, 15, 2], 0]\n",
      "[[1, 370, 566, 71, 40, 12, 5, 1051, 699, 938, 647, 2, 16, 413, 41, 1051, 15, 2], 1]\n",
      "[[1, 13, 614, 356, 75, 5, 702, 212, 134, 102, 511, 89, 340, 702, 1216, 1531, 2, 13, 3221, 75, 958, 392, 340, 702, 1216, 1531, 2], 0]\n",
      "[[1, 142, 449, 160, 938, 852, 130, 12044, 2, 160, 658, 852, 130, 2], 1]\n",
      "[[1, 7, 218, 201, 334, 4, 48, 22, 202, 444, 1051, 958, 12045, 2, 28, 218, 5, 340, 201, 334, 4, 16, 52, 801, 607, 1531, 699, 2643, 12045, 2], 1]\n",
      "[[1, 41, 936, 356, 424, 52, 1092, 95, 598, 650, 2, 13, 614, 356, 340, 9, 124, 93, 598, 650, 2], 1]\n",
      "[[1, 1051, 699, 603, 1118, 250, 196, 16, 1353, 2, 1051, 1531, 938, 647, 603, 1118, 2], 1]\n",
      "[[1, 13, 614, 356, 178, 75, 262, 793, 212, 399, 789, 109, 990, 1268, 218, 179, 65, 16, 52, 1051, 699, 4, 181, 181, 340, 9, 789, 990, 1260, 2, 142, 449, 370, 566, 82, 276, 5, 1051, 699, 212, 399, 2], 1]\n",
      "[[1, 160, 152, 201, 699, 5, 64, 59, 217, 399, 936, 356, 559, 12045, 2, 201, 699, 603, 1118, 310, 142, 449, 239, 38, 2], 1]\n",
      "[[1, 346, 608, 65, 332, 399, 2, 601, 226, 115, 5, 217, 399, 10, 65, 332, 2], 1]\n",
      "[[1, 16, 1092, 95, 936, 356, 315, 2, 16, 596, 581, 797, 40, 1051, 699, 304, 259, 2], 1]\n",
      "[[1, 836, 75, 427, 852, 130, 743, 45, 2207, 48, 22, 1114, 12045, 2, 226, 170, 4, 75, 313, 820, 332, 75, 702, 1216, 1531, 5, 1531, 699, 852, 130, 2], 0]\n",
      "[[1, 614, 356, 36, 717, 48, 22, 1051, 699, 2453, 2, 614, 356, 36, 717, 48, 22, 96, 852, 1051, 699, 2], 0]\n",
      "[[1, 142, 449, 370, 566, 702, 1216, 1531, 369, 52, 2, 702, 1216, 1531, 129, 1459, 936, 356, 445, 88, 2], 0]\n",
      "[[1, 13, 614, 356, 201, 699, 777, 500, 9, 958, 535, 160, 1519, 201, 16, 15, 12045, 2, 201, 699, 36, 4, 160, 496, 777, 103, 562, 852, 16, 581, 4, 48, 10, 777, 155, 5, 958, 265, 60, 581, 824, 15, 4, 113, 10, 201, 16, 28, 4, 936, 356, 381, 104, 2], 0]\n",
      "[[1, 4376, 340, 9, 429, 2, 13, 614, 356, 335, 16, 45, 341, 195, 1915, 269, 2], 1]\n",
      "[[1, 647, 1697, 102, 7, 86, 1915, 269, 2, 86, 195, 1915, 269, 1010, 201, 553, 125, 4, 9, 340, 104, 2], 0]\n",
      "[[1, 7, 689, 65, 876, 32, 416, 75, 128, 543, 524, 418, 2, 340, 9, 162, 449, 61, 128, 160, 1519, 2], 1]\n",
      "[[1, 370, 566, 702, 1531, 2, 48, 22, 16, 29, 600, 128, 4, 647, 370, 566, 702, 1216, 1531, 938, 647, 12043, 1183, 1183, 2], 1]\n",
      "[[1, 145, 556, 293, 45, 128, 543, 1260, 2, 1051, 699, 36, 4, 797, 40, 412, 406, 128, 543, 340, 293, 45, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 5, 702, 212, 958, 392, 155, 180, 640, 797, 40, 777, 71, 109, 49, 340, 9, 1051, 958, 47, 7, 236, 1260, 12045, 2, 14, 3384, 702, 1216, 1531, 10, 7, 50, 1114, 12045, 2], 1]\n",
      "[[1, 936, 356, 16, 52, 2, 13, 3221, 75, 238, 10, 88, 16, 15, 246, 464, 2], 1]\n",
      "[[1, 13, 614, 356, 702, 1216, 1531, 180, 71, 299, 340, 9, 852, 130, 2, 13, 614, 356, 75, 340, 852, 130, 12045, 2], 1]\n",
      "[[1, 938, 647, 49, 4, 413, 41, 128, 543, 524, 418, 4, 413, 41, 312, 309, 138, 402, 1114, 2, 113, 10, 75, 160, 39, 938, 647, 15, 86, 7, 439, 630, 370, 16, 15, 212, 399, 936, 356, 381, 104, 1091, 2], 0]\n",
      "[[1, 13, 614, 356, 75, 702, 1531, 1216, 597, 496, 75, 16, 1092, 95, 702, 1324, 797, 40, 598, 650, 41, 323, 12045, 2, 13, 614, 356, 75, 5, 10, 597, 496, 4, 226, 556, 596, 581, 702, 1216, 140, 5, 598, 650, 1260, 2], 0]\n",
      "[[1, 250, 201, 699, 797, 40, 777, 1507, 603, 15, 4, 234, 600, 16, 52, 243, 223, 201, 699, 4, 647, 358, 142, 449, 263, 806, 797, 40, 777, 500, 12045, 2, 142, 449, 459, 1137, 556, 201, 1915, 269, 2], 0]\n",
      "[[1, 135, 214, 597, 496, 75, 340, 9, 124, 93, 598, 592, 41, 323, 10, 614, 356, 221, 461, 12045, 2, 556, 124, 93, 598, 592, 936, 356, 381, 104, 2], 1]\n",
      "[[1, 75, 2717, 91, 5, 797, 40, 777, 1507, 603, 15, 4, 807, 777, 49, 777, 500, 16, 7, 314, 15, 2, 75, 313, 285, 2717, 91, 5, 797, 40, 777, 48, 22, 1114, 2], 1]\n",
      "[[1, 48, 75, 5, 247, 98, 155, 340, 9, 702, 1216, 1531, 1051, 699, 1260, 12045, 2, 297, 9, 2502, 228, 98, 424, 10, 1577, 647, 478, 246, 12047, 2], 1]\n",
      "[[1, 75, 5, 221, 461, 10, 178, 75, 508, 125, 160, 152, 201, 334, 15, 1531, 699, 4, 201, 52, 486, 1531, 1114, 2, 10, 16, 10, 2494, 699, 62, 64, 201, 71, 299, 4, 466, 28, 113, 48, 22, 1531, 39, 61, 2], 0]\n",
      "[[1, 75, 797, 40, 777, 155, 9, 958, 4, 936, 356, 201, 699, 32, 603, 1118, 12045, 2, 75, 358, 86, 4, 75, 9, 7, 277, 2342, 5, 702, 256, 1216, 1051, 1531, 4, 47, 195, 201, 699, 15, 340, 12045, 2], 1]\n",
      "[[1, 142, 449, 374, 217, 399, 2, 702, 1216, 1531, 936, 356, 559, 217, 399, 2], 0]\n",
      "[[1, 226, 170, 4, 226, 115, 5, 478, 231, 128, 543, 10, 65, 332, 12045, 2, 48, 22, 445, 128, 543, 938, 647, 160, 69, 852, 130, 1114, 2], 1]\n",
      "[[1, 2342, 59, 722, 195, 201, 7, 300, 201, 65, 332, 2, 1068, 142, 75, 1051, 3345, 949, 958, 4, 312, 27, 201, 699, 248, 82, 936, 356, 559, 5, 1091, 2], 0]\n",
      "[[1, 160, 152, 201, 699, 413, 41, 416, 217, 399, 1114, 2, 160, 152, 201, 699, 217, 399, 936, 356, 559, 2], 0]\n",
      "[[1, 936, 314, 201, 699, 2, 181, 125, 201, 958, 217, 399, 936, 356, 559, 2], 0]\n",
      "[[1, 247, 98, 3384, 958, 392, 142, 449, 1674, 120, 702, 1216, 1531, 2, 29, 348, 5, 247, 98, 48, 22, 702, 1216, 1531, 1114, 2], 0]\n",
      "[[1, 247, 98, 500, 806, 15, 4, 936, 356, 263, 285, 2, 142, 449, 263, 806, 247, 98, 500, 1086, 2], 0]\n",
      "[[1, 459, 335, 16, 45, 75, 48, 1531, 852, 130, 2, 154, 72, 335, 45, 852, 130, 2], 0]\n",
      "[[1, 263, 806, 1834, 699, 777, 2, 75, 806, 777, 15, 936, 356, 201, 2], 0]\n",
      "[[1, 201, 699, 49, 65, 876, 48, 22, 486, 1051, 2, 341, 136, 201, 699, 192, 852, 181, 541, 2], 1]\n",
      "[[1, 208, 42, 335, 16, 45, 702, 1216, 1531, 275, 374, 2, 13, 3221, 75, 852, 130, 46, 152, 7, 339, 10, 5736, 4, 508, 125, 697, 187, 282, 33, 154, 72, 1051, 958, 2], 0]\n",
      "[[1, 16, 32, 1834, 370, 75, 5, 63, 44, 453, 29, 940, 2, 13, 614, 356, 341, 218, 1531, 699, 165, 41, 185, 370, 453, 29, 2], 0]\n",
      "[[1, 52, 370, 87, 192, 1114, 2, 1051, 699, 52, 158, 797, 40, 160, 87, 1114, 2], 0]\n",
      "[[1, 13, 614, 356, 1051, 699, 598, 592, 556, 124, 93, 2, 13, 614, 356, 75, 3384, 9, 702, 1216, 1531, 535, 938, 647, 16, 15, 2], 0]\n",
      "[[1, 75, 5, 2717, 91, 797, 40, 777, 416, 2263, 141, 98, 943, 15, 2, 75, 2717, 91, 5, 797, 40, 777, 1160, 15, 4, 806, 15, 501, 777, 4, 777, 500, 282, 15, 2], 0]\n",
      "[[1, 75, 647, 358, 892, 1531, 699, 413, 41, 614, 356, 304, 259, 75, 1531, 699, 1755, 13, 614, 356, 113, 10, 16, 33, 369, 2, 702, 1216, 1531, 938, 647, 603, 1118, 250, 196, 2], 1]\n",
      "[[1, 1349, 159, 65, 876, 75, 116, 424, 48, 22, 88, 124, 4002, 2, 86, 7, 650, 10, 449, 36, 2], 1]\n",
      "[[1, 3345, 12065, 540, 42, 2802, 12062, 936, 356, 32, 43, 37, 317, 2, 88, 124, 46, 49, 9, 65, 332, 852, 130, 2], 1]\n",
      "[[1, 13, 614, 356, 702, 1216, 1531, 340, 9, 45, 1915, 2, 508, 125, 201, 340, 45, 1915, 2], 0]"
     ]
    }
   ],
   "source": [
    "# 我们按照千言文本相似度竞赛的提交格式将预测结果存储在 lcqmc.tsv 中，用来后续提交\n",
    "# 同时将预测结果输出到终端，便于大家直观感受模型预测效果\n",
    "\n",
    "# test_ds = load_dataset(\"lcqmc\", splits=[\"test\"])\n",
    "\n",
    "# with open(\"lcqmc.tsv\", 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(\"index\\tprediction\\n\")    \n",
    "#     for idx, y_pred in enumerate(y_preds):\n",
    "#         f.write(\"{}\\t{}\\n\".format(idx, y_pred))\n",
    "#         text_pair = test_ds[idx]\n",
    "#         text_pair[\"label\"] = y_pred\n",
    "#         print(text_pair)\n",
    "\n",
    "test_ds = test_ds_copy\n",
    "\n",
    "\n",
    "with open(\"bq_corpus.tsv\", 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")    \n",
    "    for idx, y_pred in enumerate(y_preds):\n",
    "        f.write(\"{}\\t{}\\n\".format(idx, y_pred))\n",
    "        text_pair = list(test_ds[idx])\n",
    "        text_pair[-1] = y_pred\n",
    "        print(text_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 提交 LCQMC 预测结果[千言文本相似度竞赛](https://aistudio.baidu.com/aistudio/competition/detail/45)\n",
    "\n",
    "千言文本相似度竞赛一共有 3 个数据集: lcqmc、bq_corpus、paws-x, 我们刚才生成了 lcqmc 的预测结果 lcqmc.tsv, 同时我们在项目内提供了 bq_corpus、paw-x 数据集的空预测结果，我们将这 3 个文件打包提交到千言文本相似度竞赛，即可看到自己的模型在 Lcqmc 数据集上的竞赛成绩。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: lcqmc.tsv (deflated 65%)\r\n",
      "updating: paws-x.tsv (deflated 64%)\r\n",
      "updating: bq_corpus.tsv (deflated 65%)\r\n"
     ]
    }
   ],
   "source": [
    "# 打包预测结果\n",
    "!zip submit.zip lcqmc.tsv paws-x.tsv bq_corpus.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 提交预测结果 submit.zip 到 [千言文本相似度竞赛](https://aistudio.baidu.com/aistudio/competition/detail/45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 千言文本相似度竞赛结果截图\n",
    "\n",
    "将自己的竞赛结果贴在此处\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ccc5814f4582493eb102d9beb02e04c0e38ffb19010d4b93ac43f021db83aff0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
