{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>!!!首先将paddlenlp更新到最新版本!!!</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/b1/e9/128dfc1371db3fc2fa883d8ef27ab6b21e3876e76750a43f58cf3c24e707/paddlenlp-2.0.2-py3-none-any.whl (426kB)\n",
      "\u001b[K     |████████████████████████████████| 430kB 13.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.1\n",
      "    Uninstalling paddlenlp-2.0.1:\n",
      "      Successfully uninstalled paddlenlp-2.0.1\n",
      "Successfully installed paddlenlp-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import paddlenlp\n",
    "\n",
    "print(paddlenlp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下载MRSA-NER数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-12 02:55:24--  https://paddlenlp.bj.bcebos.com/datasets/msra_ner.tar.gz\n",
      "Resolving paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)... 182.61.200.195, 182.61.200.229, 2409:8c00:6c21:10ad:0:ff:b00e:67d\n",
      "Connecting to paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)|182.61.200.195|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3743966 (3.6M) [application/octet-stream]\n",
      "Saving to: ‘msra_ner.tar.gz’\n",
      "\n",
      "msra_ner.tar.gz     100%[===================>]   3.57M  14.7MB/s    in 0.2s    \n",
      "\n",
      "2021-06-12 02:55:25 (14.7 MB/s) - ‘msra_ner.tar.gz’ saved [3743966/3743966]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 下载 msra-ner 数据集到当前目录\n",
    "!wget https://paddlenlp.bj.bcebos.com/datasets/msra_ner.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msra_ner/\n",
      "msra_ner/train.tsv\n",
      "msra_ner/label_map.json\n",
      "msra_ner/test.tsv\n"
     ]
    }
   ],
   "source": [
    "# 解压msra-ner数据集\n",
    "!tar -zxvf ./msra_ner.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解压后的msra_ner目录下有三个文件：train.tsv, test.tsv, label_map.json\n",
    "\n",
    "其中 train.tsv 和 test.tsv 的每一行为一个样本，样本格式为：\n",
    "\n",
    "tokens\tlabels\n",
    "\n",
    "tokens 为需要标注的样本，labels 为标注信息，二者由制表符(\\t)隔开。tokens 中的每一个字和labels中的每一个标注由STX（即\\002）字符隔开。\n",
    "\n",
    "label_map.json，即标记字典文件，存放了每一标记和标记ID的映射：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSRA-NER数据集中，训练集包含45000个样本，测试集包含3442个样本。\n",
    "\n",
    "数据集包含三类实体：人物(Person), 组织机构（Organization），地点（Location）和其他（Other），对应的标记简写为：PER，ORG，LOC和O。\n",
    "\n",
    "数据集采用BIO标注的方式，label_map.json文件，即标记字典文件，存放了每一个标记和标记ID的映射：\n",
    "\n",
    "```\n",
    "    {\n",
    "      \"B-PER\": 0,\n",
    "      \"I-PER\": 1,\n",
    "      \"B-ORG\": 2,\n",
    "      \"I-ORG\": 3,\n",
    "      \"B-LOC\": 4,\n",
    "      \"I-LOC\": 5,\n",
    "      \"O\": 6\n",
    "    }\n",
    "```\n",
    "\n",
    "每一个标记的含义如下：\n",
    "\n",
    "|标记|含义|\n",
    "|---|---|\n",
    "|B-PER|人名的开始字符|\n",
    "|I-PER|人名的非开始字符|\n",
    "|B-ORG|组织机构名的开始字符|\n",
    "|I-ORG|组织机构名的非开始字符|\n",
    "|B-LOC|地点/位置名的开始字符|\n",
    "|I-LOC|地点/位置名的非开始字符|\n",
    "|O|非命名实体部分|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集包含 45000个样本，测试集包含3442个样本，由于没有验证集，因此我们需要从训练集中划分出与测试集规模相当的验证集出来，用于在训练过程中对模型进行评估。测试集保持不变。我们选择从训练集中随机抽取3000个样本作为验证集。\n",
    "\n",
    "|数据集|样本数|\n",
    "|---|---|\n",
    "|training set| 42000|\n",
    "|dev set|3000|\n",
    "|test set|3442|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(train_file, proportion=3000):\n",
    "    \"\"\"\n",
    "    proportion: int or float\n",
    "    \"\"\"\n",
    "    with open(train_file, 'r', encoding='utf-8') as fRead:\n",
    "        train_data = []\n",
    "        for line in fRead.readlines():\n",
    "            train_data.append(line.strip('\\n'))\n",
    "        \n",
    "        data_id = list(range(len(train_data)))\n",
    "\n",
    "        train_len = len(data_id)\n",
    "        \n",
    "        if isinstance(proportion , int):\n",
    "            test_split_len = proportion\n",
    "        elif isinstance(proportion, float):\n",
    "            test_split_len = int(train_len * proportion)\n",
    "        else:\n",
    "            raise ValueError('proportion must be int or float!')\n",
    "\n",
    "\n",
    "        import random\n",
    "        random.seed(5233)\n",
    "        random.shuffle(data_id)\n",
    "\n",
    "        test_split_data = [train_data[idx] for idx in data_id[:test_split_len]]\n",
    "        train_split_data = [train_data[idx] for idx in data_id[test_split_len:]]\n",
    "\n",
    "        import os\n",
    "        data_dir = './msra_ner/data/msra'\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "\n",
    "        train_file = os.path.join(data_dir, 'train.tsv')\n",
    "        test_file = os.path.join(data_dir, 'dev.tsv')\n",
    "\n",
    "        with open(train_file, 'w', encoding='utf-8') as fWriter1:\n",
    "            for train_line in train_split_data:\n",
    "                fWriter1.write(train_line + '\\n')\n",
    "\n",
    "        with open(test_file, 'w', encoding='utf-8') as fWriter2:\n",
    "            for test_line in test_split_data:\n",
    "                fWriter2.write(test_line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split('./msra_ner/train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./msra_ner/test.tsv ./msra_ner/data/msra/test.tsv\n",
    "!cp ./msra_ner/label_map.json ./msra_ner/data/msra/label_map.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "with open('./msra_ner/data/msra/train.tsv', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        train_data.append(line)\n",
    "\n",
    "print(len(train_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "\n",
    "def read(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as fRead:\n",
    "        for line in fRead:\n",
    "            tokens, labels = line.strip('\\n').split('\\t')\n",
    "            tokens = tokens.split('\\002')  # \\002 即 STX 字符\n",
    "            labels = labels.split('\\002')\n",
    "            yield {'tokens': tokens, 'labels': labels}\n",
    "\n",
    "data_dir = './msra_ner/data/msra'\n",
    "train_file = os.path.join(data_dir, 'train.tsv')\n",
    "dev_file = os.path.join(data_dir, 'dev.tsv')\n",
    "test_file = os.path.join(data_dir, 'test.tsv')\n",
    "\n",
    "train_dataset = load_dataset(read, data_path=train_file, lazy=False)\n",
    "dev_dataset = load_dataset(read, data_path=dev_file, lazy=False)\n",
    "test_dataset = load_dataset(read, data_path=test_file, lazy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['苗', '苗', '妈', '妈', '给', '孩', '子', '以', '信', '任', '和', '期', '待', '，', '并', '且', '巧', '妙', '地', '从', '孩', '子', '力', '所', '能', '及', '的', '日', '常', '生', '活', '小', '事', '出', '发', '，', '让', '孩', '子', '在', '劳', '动', '中', '体', '验', '生', '活', '的', '苦', '与', '乐', '。'], 'labels': ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "{'tokens': ['郎', '青', '山', '亲', '自', '抓', '产', '供', '销', '和', '技', '术', '改', '造', '，', '副', '总', '经', '理', '分', '管', '财', '务', '、', '后', '勤', '和', '生', '产', '调', '度', '，', '其', '他', '人', '负', '责', '技', '术', '监', '督', '。'], 'labels': ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "{'tokens': ['日', '本', '官', '方', '近', '来', '就', '日', '美', '防', '卫', '合', '作', '指', '针', '的', '适', '用', '范', '围', '是', '否', '包', '括', '中', '国', '台', '湾', '，', '发', '表', '了', '自', '相', '矛', '盾', '的', '言', '论', '，', '遭', '到', '中', '国', '舆', '论', '的', '谴', '责', '。'], 'labels': ['B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "for idx, ex in enumerate(train_dataset):\n",
    "    if idx < 3:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-PER': 0, 'I-PER': 1, 'B-ORG': 2, 'I-ORG': 3, 'B-LOC': 4, 'I-LOC': 5, 'O': 6}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('./msra_ner/data/msra/label_map.json', 'r', encoding='utf-8') as fRead:\n",
    "    label_vocab = json.load(fRead)\n",
    "\n",
    "print(label_vocab)\n",
    "\n",
    "print(len(label_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建BERT输入特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 02:55:52,819] [    INFO] - Downloading bert-base-chinese-vocab.txt from https://paddle-hapi.bj.bcebos.com/models/bert/bert-base-chinese-vocab.txt\n",
      "100%|██████████| 107/107 [00:00<00:00, 3318.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 5728, 5728, 1968, 1968, 5314, 2111, 2094, 809, 928, 818, 1469, 3309, 2521, 8024, 2400, 684, 2341, 1975, 1765, 794, 2111, 2094, 1213, 2792, 5543, 1350, 4638, 3189, 2382, 4495, 3833, 2207, 752, 1139, 1355, 8024, 6375, 2111, 2094, 1762, 1227, 1220, 704, 860, 7741, 4495, 3833, 4638, 5736, 680, 727, 511, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 54, [6, 0, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def convert_example(example, tokenizer, label_vocab, max_seq_len=512):\n",
    "    tokens, labels = example['tokens'], example['labels']\n",
    "    \n",
    "    tokenized_inputs = tokenizer(tokens, return_length=True, is_split_into_words=True, max_seq_len=max_seq_len)\n",
    "\n",
    "    labels = [label_vocab[label] for label in labels]\n",
    "    if len(labels) + 2 > max_seq_len:\n",
    "        labels = labels[: max_seq_len - 2] \n",
    "\n",
    "    tokenized_inputs['labels'] = [label_vocab['O']] + labels + [label_vocab['O']] \n",
    "\n",
    "    return tokenized_inputs['input_ids'], tokenized_inputs['token_type_ids'], tokenized_inputs['seq_len'], tokenized_inputs['labels']\n",
    "\n",
    "for idx, example in enumerate(train_dataset):\n",
    "    if idx < 1:\n",
    "        print(convert_example(example, tokenizer, label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddlenlp.datasets.dataset.MapDataset at 0x7f9f914a6850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\n",
    "\n",
    "train_dataset.map(trans_func)\n",
    "dev_dataset.map(trans_func)\n",
    "test_dataset.map(trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 5728, 5728, 1968, 1968, 5314, 2111, 2094, 809, 928, 818, 1469, 3309, 2521, 8024, 2400, 684, 2341, 1975, 1765, 794, 2111, 2094, 1213, 2792, 5543, 1350, 4638, 3189, 2382, 4495, 3833, 2207, 752, 1139, 1355, 8024, 6375, 2111, 2094, 1762, 1227, 1220, 704, 860, 7741, 4495, 3833, 4638, 5736, 680, 727, 511, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 54, [6, 0, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n",
      "([101, 6947, 7471, 2255, 779, 5632, 2831, 772, 897, 7218, 1469, 2825, 3318, 3121, 6863, 8024, 1199, 2600, 5307, 4415, 1146, 5052, 6568, 1218, 510, 1400, 1249, 1469, 4495, 772, 6444, 2428, 8024, 1071, 800, 782, 6566, 6569, 2825, 3318, 4664, 4719, 511, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 44, [6, 0, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n",
      "([101, 3189, 3315, 2135, 3175, 6818, 3341, 2218, 3189, 5401, 7344, 1310, 1394, 868, 2900, 7151, 4638, 6844, 4500, 5745, 1741, 3221, 1415, 1259, 2886, 704, 1744, 1378, 3968, 8024, 1355, 6134, 749, 5632, 4685, 4757, 4688, 4638, 6241, 6389, 8024, 6901, 1168, 704, 1744, 5644, 6389, 4638, 6482, 6569, 511, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 52, [6, 4, 5, 6, 6, 6, 6, 6, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "for idx, example in enumerate(train_dataset):\n",
    "    if idx < 3:\n",
    "        print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建数据集加载器 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle.io import DataLoader\n",
    "from paddle.io import DistributedBatchSampler\n",
    "from paddle.io import BatchSampler\n",
    "from paddlenlp.data import Pad\n",
    "from paddlenlp.data import Stack\n",
    "from paddlenlp.data import Tuple\n",
    "\n",
    "\n",
    "pad_label_id = -1\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id), # token_type_ids\n",
    "    Stack(), # seq_len\n",
    "    Pad(axis=0, pad_val=pad_label_id)  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "train_batch_sampler = DistributedBatchSampler(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_sampler=train_batch_sampler, \n",
    "    collate_fn=batchify_fn, \n",
    "    return_list=True\n",
    ")\n",
    "\n",
    "dev_batch_sampler = BatchSampler(\n",
    "    dataset=dev_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "dev_data_loader = DataLoader(\n",
    "    dataset=dev_dataset,\n",
    "    batch_sampler=dev_batch_sampler,\n",
    "    collate_fn=batchify_fn,\n",
    "    return_list=True\n",
    ")\n",
    "\n",
    "test_batch_sampler = BatchSampler(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=batchify_fn,\n",
    "    return_list=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型 ：加载 BERT 预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 02:56:06,020] [    INFO] - Downloading http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams and saved to /home/aistudio/.paddlenlp/models/bert-base-chinese\n",
      "[2021-06-12 02:56:06,071] [    INFO] - Downloading bert-base-chinese.pdparams from http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams\n",
      "100%|██████████| 696494/696494 [00:10<00:00, 66666.58it/s]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import BertForTokenClassification\n",
    "\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-chinese', num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练与评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle.nn import CrossEntropyLoss\n",
    "from paddle.optimizer import AdamW\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=pad_label_id)\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=False)\n",
    "optimizer = AdamW(learning_rate=2e-5, parameters=model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model,loss_fn, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch_data in data_loader:\n",
    "\n",
    "        input_ids, token_type_ids, length, labels = batch_data\n",
    "\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        preds = paddle.argmax(logits, axis=-1)\n",
    "        n_infer, n_label, n_correct = metric.compute(None, length, preds, labels)\n",
    "        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\n",
    "        precission, recall, f1_score = metric.accumulate()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return np.mean(losses), precission, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:239: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.bool, the right dtype will convert to paddle.float32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "[2021-06-12 03:00:04,273] [ WARNING] - Compatibility Warning: The params of ChunkEvaluator.compute has been modified. The old version is `inputs`, `lengths`, `predictions`, `labels` while the current version is `lengths`, `predictions`, `labels`.  Please update the usage.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 10, epoch: 0, batch: 9, loss: 0.37573, precission: 0.00213, recall: 0.00668, f1_score: 0.00323, speed: 4.27 step/s\n",
      "global_step: 20, epoch: 0, batch: 19, loss: 0.39082, precission: 0.00213, recall: 0.00325, f1_score: 0.00257, speed: 3.85 step/s\n",
      "global_step: 30, epoch: 0, batch: 29, loss: 0.22917, precission: 0.00258, recall: 0.00281, f1_score: 0.00269, speed: 4.85 step/s\n",
      "global_step: 40, epoch: 0, batch: 39, loss: 0.23283, precission: 0.03394, recall: 0.03645, f1_score: 0.03515, speed: 4.72 step/s\n",
      "global_step: 50, epoch: 0, batch: 49, loss: 0.11737, precission: 0.11885, recall: 0.13220, f1_score: 0.12517, speed: 4.56 step/s\n",
      "global_step: 60, epoch: 0, batch: 59, loss: 0.09801, precission: 0.19408, recall: 0.21962, f1_score: 0.20606, speed: 4.06 step/s\n",
      "global_step: 70, epoch: 0, batch: 69, loss: 0.11979, precission: 0.26075, recall: 0.29829, f1_score: 0.27826, speed: 3.65 step/s\n",
      "global_step: 80, epoch: 0, batch: 79, loss: 0.11220, precission: 0.29760, recall: 0.34032, f1_score: 0.31753, speed: 4.40 step/s\n",
      "global_step: 90, epoch: 0, batch: 89, loss: 0.11053, precission: 0.34331, recall: 0.39232, f1_score: 0.36619, speed: 3.84 step/s\n",
      "global_step: 100, epoch: 0, batch: 99, loss: 0.05155, precission: 0.37929, recall: 0.43195, f1_score: 0.40391, speed: 4.51 step/s\n",
      "eval dev loss: 0.05540, precission: 0.76853, recall: 0.82323, f1_score: 0.79494\n",
      "save model at global step : 100, best val f1_score: 0.79494\n",
      "global_step: 110, epoch: 0, batch: 109, loss: 0.04691, precission: 0.76360, recall: 0.81906, f1_score: 0.79036, speed: 0.62 step/s\n",
      "global_step: 120, epoch: 0, batch: 119, loss: 0.07290, precission: 0.75831, recall: 0.81708, f1_score: 0.78660, speed: 4.21 step/s\n",
      "global_step: 130, epoch: 0, batch: 129, loss: 0.05537, precission: 0.76111, recall: 0.81796, f1_score: 0.78851, speed: 4.66 step/s\n",
      "global_step: 140, epoch: 0, batch: 139, loss: 0.04078, precission: 0.76154, recall: 0.81817, f1_score: 0.78884, speed: 4.26 step/s\n",
      "global_step: 150, epoch: 0, batch: 149, loss: 0.04734, precission: 0.76260, recall: 0.82057, f1_score: 0.79052, speed: 3.51 step/s\n",
      "global_step: 160, epoch: 0, batch: 159, loss: 0.02885, precission: 0.76348, recall: 0.82146, f1_score: 0.79141, speed: 4.60 step/s\n",
      "global_step: 170, epoch: 0, batch: 169, loss: 0.03694, precission: 0.76431, recall: 0.82190, f1_score: 0.79206, speed: 3.63 step/s\n",
      "global_step: 180, epoch: 0, batch: 179, loss: 0.03432, precission: 0.76552, recall: 0.82187, f1_score: 0.79270, speed: 3.71 step/s\n",
      "global_step: 190, epoch: 0, batch: 189, loss: 0.06452, precission: 0.76754, recall: 0.82322, f1_score: 0.79440, speed: 4.14 step/s\n",
      "global_step: 200, epoch: 0, batch: 199, loss: 0.02113, precission: 0.76787, recall: 0.82493, f1_score: 0.79538, speed: 3.63 step/s\n",
      "eval dev loss: 0.03900, precission: 0.83594, recall: 0.88078, f1_score: 0.85777\n",
      "save model at global step : 200, best val f1_score: 0.85777\n",
      "global_step: 210, epoch: 0, batch: 209, loss: 0.02836, precission: 0.83053, recall: 0.87634, f1_score: 0.85282, speed: 0.60 step/s\n",
      "global_step: 220, epoch: 0, batch: 219, loss: 0.06488, precission: 0.82718, recall: 0.87185, f1_score: 0.84892, speed: 4.70 step/s\n",
      "global_step: 230, epoch: 0, batch: 229, loss: 0.05682, precission: 0.82563, recall: 0.87112, f1_score: 0.84777, speed: 3.01 step/s\n",
      "global_step: 240, epoch: 0, batch: 239, loss: 0.03559, precission: 0.82420, recall: 0.87160, f1_score: 0.84724, speed: 3.19 step/s\n",
      "global_step: 250, epoch: 0, batch: 249, loss: 0.06881, precission: 0.81984, recall: 0.86872, f1_score: 0.84357, speed: 4.24 step/s\n",
      "global_step: 260, epoch: 0, batch: 259, loss: 0.01356, precission: 0.82196, recall: 0.86981, f1_score: 0.84521, speed: 4.08 step/s\n",
      "global_step: 270, epoch: 0, batch: 269, loss: 0.01496, precission: 0.82410, recall: 0.87022, f1_score: 0.84653, speed: 3.07 step/s\n",
      "global_step: 280, epoch: 0, batch: 279, loss: 0.04199, precission: 0.82568, recall: 0.87154, f1_score: 0.84799, speed: 4.57 step/s\n",
      "global_step: 290, epoch: 0, batch: 289, loss: 0.06486, precission: 0.82301, recall: 0.87039, f1_score: 0.84603, speed: 4.48 step/s\n",
      "global_step: 300, epoch: 0, batch: 299, loss: 0.03870, precission: 0.82329, recall: 0.87046, f1_score: 0.84622, speed: 4.71 step/s\n",
      "eval dev loss: 0.03569, precission: 0.83689, recall: 0.88263, f1_score: 0.85915\n",
      "save model at global step : 300, best val f1_score: 0.85915\n",
      "global_step: 310, epoch: 0, batch: 309, loss: 0.01694, precission: 0.83647, recall: 0.88158, f1_score: 0.85843, speed: 0.62 step/s\n",
      "global_step: 320, epoch: 0, batch: 319, loss: 0.03347, precission: 0.83614, recall: 0.88158, f1_score: 0.85826, speed: 4.60 step/s\n",
      "global_step: 330, epoch: 0, batch: 329, loss: 0.03533, precission: 0.83850, recall: 0.88155, f1_score: 0.85949, speed: 4.55 step/s\n",
      "global_step: 340, epoch: 0, batch: 339, loss: 0.03065, precission: 0.83748, recall: 0.88139, f1_score: 0.85888, speed: 4.48 step/s\n",
      "global_step: 350, epoch: 0, batch: 349, loss: 0.04556, precission: 0.83895, recall: 0.88206, f1_score: 0.85996, speed: 3.19 step/s\n",
      "global_step: 360, epoch: 0, batch: 359, loss: 0.02719, precission: 0.83802, recall: 0.88237, f1_score: 0.85963, speed: 3.41 step/s\n",
      "global_step: 370, epoch: 0, batch: 369, loss: 0.03326, precission: 0.83838, recall: 0.88234, f1_score: 0.85980, speed: 4.39 step/s\n",
      "global_step: 380, epoch: 0, batch: 379, loss: 0.04373, precission: 0.83804, recall: 0.88230, f1_score: 0.85960, speed: 4.31 step/s\n",
      "global_step: 390, epoch: 0, batch: 389, loss: 0.01400, precission: 0.83922, recall: 0.88204, f1_score: 0.86010, speed: 4.32 step/s\n",
      "global_step: 400, epoch: 0, batch: 399, loss: 0.04482, precission: 0.83961, recall: 0.88233, f1_score: 0.86044, speed: 4.16 step/s\n",
      "eval dev loss: 0.02875, precission: 0.87123, recall: 0.91832, f1_score: 0.89416\n",
      "save model at global step : 400, best val f1_score: 0.89416\n",
      "global_step: 410, epoch: 0, batch: 409, loss: 0.02038, precission: 0.86839, recall: 0.91447, f1_score: 0.89084, speed: 0.61 step/s\n",
      "global_step: 420, epoch: 0, batch: 419, loss: 0.03031, precission: 0.86635, recall: 0.91225, f1_score: 0.88871, speed: 3.42 step/s\n",
      "global_step: 430, epoch: 0, batch: 429, loss: 0.04924, precission: 0.86204, recall: 0.90669, f1_score: 0.88380, speed: 3.60 step/s\n",
      "global_step: 440, epoch: 0, batch: 439, loss: 0.02646, precission: 0.85974, recall: 0.90567, f1_score: 0.88211, speed: 4.32 step/s\n",
      "global_step: 450, epoch: 0, batch: 449, loss: 0.01548, precission: 0.85909, recall: 0.90313, f1_score: 0.88056, speed: 4.66 step/s\n",
      "global_step: 460, epoch: 0, batch: 459, loss: 0.02544, precission: 0.85920, recall: 0.90292, f1_score: 0.88052, speed: 4.31 step/s\n",
      "global_step: 470, epoch: 0, batch: 469, loss: 0.03549, precission: 0.85893, recall: 0.90253, f1_score: 0.88019, speed: 4.53 step/s\n",
      "global_step: 480, epoch: 0, batch: 479, loss: 0.03060, precission: 0.86010, recall: 0.90338, f1_score: 0.88121, speed: 4.07 step/s\n",
      "global_step: 490, epoch: 0, batch: 489, loss: 0.03761, precission: 0.86080, recall: 0.90278, f1_score: 0.88129, speed: 4.01 step/s\n",
      "global_step: 500, epoch: 0, batch: 499, loss: 0.02246, precission: 0.85967, recall: 0.90257, f1_score: 0.88060, speed: 4.14 step/s\n",
      "eval dev loss: 0.02495, precission: 0.90231, recall: 0.92017, f1_score: 0.91115\n",
      "save model at global step : 500, best val f1_score: 0.91115\n",
      "global_step: 510, epoch: 0, batch: 509, loss: 0.02821, precission: 0.89393, recall: 0.91364, f1_score: 0.90368, speed: 0.62 step/s\n",
      "global_step: 520, epoch: 0, batch: 519, loss: 0.05777, precission: 0.88772, recall: 0.90927, f1_score: 0.89837, speed: 3.96 step/s\n",
      "global_step: 530, epoch: 0, batch: 529, loss: 0.07603, precission: 0.88114, recall: 0.90401, f1_score: 0.89243, speed: 3.68 step/s\n",
      "global_step: 540, epoch: 0, batch: 539, loss: 0.02017, precission: 0.88152, recall: 0.90489, f1_score: 0.89305, speed: 4.13 step/s\n",
      "global_step: 550, epoch: 0, batch: 549, loss: 0.03544, precission: 0.87996, recall: 0.90297, f1_score: 0.89132, speed: 4.23 step/s\n",
      "global_step: 560, epoch: 0, batch: 559, loss: 0.04265, precission: 0.88129, recall: 0.90557, f1_score: 0.89327, speed: 4.62 step/s\n",
      "global_step: 570, epoch: 0, batch: 569, loss: 0.02757, precission: 0.87898, recall: 0.90442, f1_score: 0.89152, speed: 4.31 step/s\n",
      "global_step: 580, epoch: 0, batch: 579, loss: 0.07269, precission: 0.87747, recall: 0.90413, f1_score: 0.89060, speed: 2.63 step/s\n",
      "global_step: 590, epoch: 0, batch: 589, loss: 0.03859, precission: 0.87818, recall: 0.90579, f1_score: 0.89177, speed: 4.32 step/s\n",
      "global_step: 600, epoch: 0, batch: 599, loss: 0.01064, precission: 0.87764, recall: 0.90545, f1_score: 0.89133, speed: 4.32 step/s\n",
      "eval dev loss: 0.02765, precission: 0.90008, recall: 0.91790, f1_score: 0.90891\n",
      "global_step: 610, epoch: 0, batch: 609, loss: 0.01616, precission: 0.90015, recall: 0.91784, f1_score: 0.90891, speed: 0.93 step/s\n",
      "global_step: 620, epoch: 0, batch: 619, loss: 0.02577, precission: 0.89663, recall: 0.91783, f1_score: 0.90711, speed: 4.67 step/s\n",
      "global_step: 630, epoch: 0, batch: 629, loss: 0.01821, precission: 0.89804, recall: 0.91915, f1_score: 0.90847, speed: 2.89 step/s\n",
      "global_step: 640, epoch: 0, batch: 639, loss: 0.01216, precission: 0.89444, recall: 0.91768, f1_score: 0.90591, speed: 3.54 step/s\n",
      "global_step: 650, epoch: 0, batch: 649, loss: 0.03931, precission: 0.89174, recall: 0.91557, f1_score: 0.90350, speed: 3.13 step/s\n",
      "global_step: 660, epoch: 0, batch: 659, loss: 0.03993, precission: 0.89059, recall: 0.91526, f1_score: 0.90276, speed: 4.45 step/s\n",
      "global_step: 670, epoch: 0, batch: 669, loss: 0.03793, precission: 0.88792, recall: 0.91312, f1_score: 0.90034, speed: 4.04 step/s\n",
      "global_step: 680, epoch: 0, batch: 679, loss: 0.04533, precission: 0.88857, recall: 0.91249, f1_score: 0.90037, speed: 4.24 step/s\n",
      "global_step: 690, epoch: 0, batch: 689, loss: 0.01106, precission: 0.88495, recall: 0.91118, f1_score: 0.89787, speed: 3.35 step/s\n",
      "global_step: 700, epoch: 0, batch: 699, loss: 0.03638, precission: 0.88542, recall: 0.91055, f1_score: 0.89781, speed: 4.25 step/s\n",
      "eval dev loss: 0.02293, precission: 0.91568, recall: 0.92739, f1_score: 0.92150\n",
      "save model at global step : 700, best val f1_score: 0.92150\n",
      "global_step: 710, epoch: 0, batch: 709, loss: 0.01884, precission: 0.91472, recall: 0.92774, f1_score: 0.92119, speed: 0.59 step/s\n",
      "global_step: 720, epoch: 0, batch: 719, loss: 0.01911, precission: 0.91115, recall: 0.92474, f1_score: 0.91789, speed: 4.72 step/s\n",
      "global_step: 730, epoch: 0, batch: 729, loss: 0.02761, precission: 0.90906, recall: 0.92421, f1_score: 0.91657, speed: 3.38 step/s\n",
      "global_step: 740, epoch: 0, batch: 739, loss: 0.03083, precission: 0.90786, recall: 0.92250, f1_score: 0.91512, speed: 3.58 step/s\n",
      "global_step: 750, epoch: 0, batch: 749, loss: 0.01457, precission: 0.90710, recall: 0.92288, f1_score: 0.91492, speed: 3.10 step/s\n",
      "global_step: 760, epoch: 0, batch: 759, loss: 0.00707, precission: 0.90541, recall: 0.92296, f1_score: 0.91410, speed: 4.80 step/s\n",
      "global_step: 770, epoch: 0, batch: 769, loss: 0.01827, precission: 0.90280, recall: 0.92025, f1_score: 0.91144, speed: 4.31 step/s\n",
      "global_step: 780, epoch: 0, batch: 779, loss: 0.01143, precission: 0.90160, recall: 0.92071, f1_score: 0.91106, speed: 4.75 step/s\n",
      "global_step: 790, epoch: 0, batch: 789, loss: 0.03461, precission: 0.90200, recall: 0.92123, f1_score: 0.91151, speed: 3.59 step/s\n",
      "global_step: 800, epoch: 0, batch: 799, loss: 0.01785, precission: 0.90044, recall: 0.92054, f1_score: 0.91038, speed: 3.44 step/s\n",
      "eval dev loss: 0.02242, precission: 0.90791, recall: 0.93956, f1_score: 0.92347\n",
      "save model at global step : 800, best val f1_score: 0.92347\n",
      "global_step: 810, epoch: 0, batch: 809, loss: 0.01438, precission: 0.90350, recall: 0.93607, f1_score: 0.91950, speed: 0.63 step/s\n",
      "global_step: 820, epoch: 0, batch: 819, loss: 0.01804, precission: 0.89967, recall: 0.93297, f1_score: 0.91602, speed: 4.46 step/s\n",
      "global_step: 830, epoch: 0, batch: 829, loss: 0.02032, precission: 0.89867, recall: 0.93230, f1_score: 0.91517, speed: 3.33 step/s\n",
      "global_step: 840, epoch: 0, batch: 839, loss: 0.02855, precission: 0.89547, recall: 0.93007, f1_score: 0.91245, speed: 4.03 step/s\n",
      "global_step: 850, epoch: 0, batch: 849, loss: 0.02020, precission: 0.89233, recall: 0.92625, f1_score: 0.90897, speed: 2.61 step/s\n",
      "global_step: 860, epoch: 0, batch: 859, loss: 0.01770, precission: 0.89104, recall: 0.92512, f1_score: 0.90776, speed: 4.85 step/s\n",
      "global_step: 870, epoch: 0, batch: 869, loss: 0.06243, precission: 0.89117, recall: 0.92616, f1_score: 0.90833, speed: 3.56 step/s\n",
      "global_step: 880, epoch: 0, batch: 879, loss: 0.03901, precission: 0.89156, recall: 0.92482, f1_score: 0.90789, speed: 3.92 step/s\n",
      "global_step: 890, epoch: 0, batch: 889, loss: 0.02122, precission: 0.89214, recall: 0.92541, f1_score: 0.90847, speed: 4.38 step/s\n",
      "global_step: 900, epoch: 0, batch: 899, loss: 0.03301, precission: 0.89149, recall: 0.92507, f1_score: 0.90797, speed: 3.10 step/s\n",
      "eval dev loss: 0.01968, precission: 0.92080, recall: 0.93771, f1_score: 0.92918\n",
      "save model at global step : 900, best val f1_score: 0.92918\n",
      "global_step: 910, epoch: 0, batch: 909, loss: 0.00678, precission: 0.91773, recall: 0.93678, f1_score: 0.92716, speed: 0.64 step/s\n",
      "global_step: 920, epoch: 0, batch: 919, loss: 0.01022, precission: 0.91730, recall: 0.93750, f1_score: 0.92729, speed: 3.24 step/s\n",
      "global_step: 930, epoch: 0, batch: 929, loss: 0.03820, precission: 0.91432, recall: 0.93405, f1_score: 0.92408, speed: 4.36 step/s\n",
      "global_step: 940, epoch: 0, batch: 939, loss: 0.01101, precission: 0.91218, recall: 0.93336, f1_score: 0.92265, speed: 3.64 step/s\n",
      "global_step: 950, epoch: 0, batch: 949, loss: 0.02405, precission: 0.90981, recall: 0.93179, f1_score: 0.92067, speed: 3.49 step/s\n",
      "global_step: 960, epoch: 0, batch: 959, loss: 0.02589, precission: 0.90824, recall: 0.93092, f1_score: 0.91944, speed: 4.21 step/s\n",
      "global_step: 970, epoch: 0, batch: 969, loss: 0.01409, precission: 0.90917, recall: 0.93092, f1_score: 0.91992, speed: 3.72 step/s\n",
      "global_step: 980, epoch: 0, batch: 979, loss: 0.09783, precission: 0.90918, recall: 0.93142, f1_score: 0.92017, speed: 3.54 step/s\n",
      "global_step: 990, epoch: 0, batch: 989, loss: 0.01999, precission: 0.90933, recall: 0.93124, f1_score: 0.92016, speed: 4.75 step/s\n",
      "global_step: 1000, epoch: 0, batch: 999, loss: 0.02250, precission: 0.91013, recall: 0.93195, f1_score: 0.92092, speed: 3.39 step/s\n",
      "eval dev loss: 0.02022, precission: 0.92476, recall: 0.94307, f1_score: 0.93382\n",
      "save model at global step : 1000, best val f1_score: 0.93382\n",
      "global_step: 1010, epoch: 0, batch: 1009, loss: 0.00813, precission: 0.92245, recall: 0.94104, f1_score: 0.93166, speed: 0.64 step/s\n",
      "global_step: 1020, epoch: 0, batch: 1019, loss: 0.00680, precission: 0.92304, recall: 0.94187, f1_score: 0.93236, speed: 3.63 step/s\n",
      "global_step: 1030, epoch: 0, batch: 1029, loss: 0.01660, precission: 0.91985, recall: 0.94122, f1_score: 0.93042, speed: 4.45 step/s\n",
      "global_step: 1040, epoch: 0, batch: 1039, loss: 0.01032, precission: 0.91801, recall: 0.94098, f1_score: 0.92935, speed: 3.30 step/s\n",
      "global_step: 1050, epoch: 0, batch: 1049, loss: 0.05022, precission: 0.91617, recall: 0.93841, f1_score: 0.92715, speed: 3.30 step/s\n",
      "global_step: 1060, epoch: 0, batch: 1059, loss: 0.01741, precission: 0.91556, recall: 0.93761, f1_score: 0.92645, speed: 4.19 step/s\n",
      "global_step: 1070, epoch: 0, batch: 1069, loss: 0.02818, precission: 0.91464, recall: 0.93711, f1_score: 0.92574, speed: 4.08 step/s\n",
      "global_step: 1080, epoch: 0, batch: 1079, loss: 0.00728, precission: 0.91611, recall: 0.93752, f1_score: 0.92669, speed: 3.45 step/s\n",
      "global_step: 1090, epoch: 0, batch: 1089, loss: 0.02616, precission: 0.91612, recall: 0.93724, f1_score: 0.92656, speed: 2.88 step/s\n",
      "global_step: 1100, epoch: 0, batch: 1099, loss: 0.03971, precission: 0.91562, recall: 0.93680, f1_score: 0.92609, speed: 4.28 step/s\n",
      "eval dev loss: 0.01862, precission: 0.92688, recall: 0.94389, f1_score: 0.93531\n",
      "save model at global step : 1100, best val f1_score: 0.93531\n",
      "global_step: 1110, epoch: 0, batch: 1109, loss: 0.02416, precission: 0.92337, recall: 0.94201, f1_score: 0.93259, speed: 0.64 step/s\n",
      "global_step: 1120, epoch: 0, batch: 1119, loss: 0.02841, precission: 0.92263, recall: 0.94138, f1_score: 0.93191, speed: 4.41 step/s\n",
      "global_step: 1130, epoch: 0, batch: 1129, loss: 0.01637, precission: 0.91764, recall: 0.93895, f1_score: 0.92818, speed: 3.43 step/s\n",
      "global_step: 1140, epoch: 0, batch: 1139, loss: 0.01322, precission: 0.91586, recall: 0.93730, f1_score: 0.92645, speed: 4.51 step/s\n",
      "global_step: 1150, epoch: 0, batch: 1149, loss: 0.01792, precission: 0.91431, recall: 0.93593, f1_score: 0.92499, speed: 3.48 step/s\n",
      "global_step: 1160, epoch: 0, batch: 1159, loss: 0.03663, precission: 0.91215, recall: 0.93495, f1_score: 0.92341, speed: 4.60 step/s\n",
      "global_step: 1170, epoch: 0, batch: 1169, loss: 0.03138, precission: 0.91038, recall: 0.93403, f1_score: 0.92206, speed: 4.53 step/s\n",
      "global_step: 1180, epoch: 0, batch: 1179, loss: 0.02310, precission: 0.90991, recall: 0.93362, f1_score: 0.92162, speed: 4.26 step/s\n",
      "global_step: 1190, epoch: 0, batch: 1189, loss: 0.01095, precission: 0.90825, recall: 0.93394, f1_score: 0.92092, speed: 3.94 step/s\n",
      "global_step: 1200, epoch: 0, batch: 1199, loss: 0.02315, precission: 0.90820, recall: 0.93431, f1_score: 0.92107, speed: 2.46 step/s\n",
      "eval dev loss: 0.01908, precission: 0.93684, recall: 0.93936, f1_score: 0.93810\n",
      "save model at global step : 1200, best val f1_score: 0.93810\n",
      "global_step: 1210, epoch: 0, batch: 1209, loss: 0.05440, precission: 0.93543, recall: 0.93771, f1_score: 0.93657, speed: 0.64 step/s\n",
      "global_step: 1220, epoch: 0, batch: 1219, loss: 0.01882, precission: 0.93210, recall: 0.93666, f1_score: 0.93438, speed: 2.65 step/s\n",
      "global_step: 1230, epoch: 0, batch: 1229, loss: 0.00997, precission: 0.93097, recall: 0.93713, f1_score: 0.93404, speed: 4.55 step/s\n",
      "global_step: 1240, epoch: 0, batch: 1239, loss: 0.04752, precission: 0.93017, recall: 0.93679, f1_score: 0.93347, speed: 3.44 step/s\n",
      "global_step: 1250, epoch: 0, batch: 1249, loss: 0.05481, precission: 0.92750, recall: 0.93515, f1_score: 0.93131, speed: 3.92 step/s\n",
      "global_step: 1260, epoch: 0, batch: 1259, loss: 0.00937, precission: 0.92727, recall: 0.93631, f1_score: 0.93177, speed: 4.37 step/s\n",
      "global_step: 1270, epoch: 0, batch: 1269, loss: 0.01555, precission: 0.92669, recall: 0.93574, f1_score: 0.93119, speed: 3.74 step/s\n",
      "global_step: 1280, epoch: 0, batch: 1279, loss: 0.02120, precission: 0.92572, recall: 0.93520, f1_score: 0.93044, speed: 4.17 step/s\n",
      "global_step: 1290, epoch: 0, batch: 1289, loss: 0.03814, precission: 0.92380, recall: 0.93497, f1_score: 0.92935, speed: 4.18 step/s\n",
      "global_step: 1300, epoch: 0, batch: 1299, loss: 0.01514, precission: 0.92306, recall: 0.93384, f1_score: 0.92842, speed: 4.40 step/s\n",
      "eval dev loss: 0.02141, precission: 0.92416, recall: 0.93007, f1_score: 0.92711\n",
      "global_step: 1310, epoch: 0, batch: 1309, loss: 0.00753, precission: 0.92212, recall: 0.92956, f1_score: 0.92583, speed: 0.93 step/s\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "epochs = 1\n",
    "best_f1_score = 0.0\n",
    "best_precission = 0.0\n",
    "best_recall = 0.0\n",
    "\n",
    "print_every_step = 10\n",
    "evaluate_every_step = 100\n",
    "\n",
    "save_dir = os.path.join('./data', 'checkpoints')\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_param_path = os.path.join(save_dir, 'bets_model_state.pdparams')\n",
    "\n",
    "tic_train = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch_data in enumerate(train_data_loader):\n",
    "        \n",
    "        input_ids, token_type_ids, length, labels = batch_data\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        preds = paddle.argmax(logits, axis=-1)\n",
    "        n_infer, n_label, n_correct = metric.compute(None, length, preds, labels)\n",
    "        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\n",
    "        precission, recall, f1_score = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % print_every_step == 0:\n",
    "            print('global_step: %d, epoch: %d, batch: %d, loss: %.5f, precission: %.5f, recall: %.5f, f1_score: %.5f, speed: %.2f step/s' % (\n",
    "                global_step, epoch, step, loss.numpy(), precission, recall, f1_score, print_every_step / (time.time() - tic_train)\n",
    "            ))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % evaluate_every_step == 0:\n",
    "            loss, precission, recall, f1_score = evaluate(model, loss_fn, metric, dev_data_loader)\n",
    "            print('eval dev loss: %.5f, precission: %.5f, recall: %.5f, f1_score: %.5f' % (\n",
    "                loss, precission, recall, f1_score\n",
    "            ))\n",
    "            if f1_score > best_f1_score and precission > best_precission and recall > best_recall:\n",
    "                \n",
    "                best_f1_score = f1_score\n",
    "                best_precission = precission \n",
    "                best_recall = best_recall:\n",
    "\n",
    "                print('save model at global step : %d, best_precission: %.5f, best_recall: %.5f, best val f1_score: %.5f' % (\n",
    "                    global_step, best_precission, best_recall, best_f1_score\n",
    "                ))\n",
    "\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, ds, label_vocab):\n",
    "    pred_list = []\n",
    "    len_list = []\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        logits = model(input_ids, seg_ids)\n",
    "        pred = paddle.argmax(logits, axis=-1)\n",
    "        pred_list.append(pred.numpy())\n",
    "        len_list.append(lens.numpy())\n",
    "\n",
    "    return pred_list, len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list, len_list = predict(model, test_data_loader, test_dataset, label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-PER': 0,\n",
       " 'I-PER': 1,\n",
       " 'B-ORG': 2,\n",
       " 'I-ORG': 3,\n",
       " 'B-LOC': 4,\n",
       " 'I-LOC': 5,\n",
       " 'O': 6}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold = 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 5, 5, 3, 3, 6,\n",
       "       6, 6, 6, 6, 4, 5, 5, 5, 5, 5, 6, 6, 0, 1, 1, 6, 6, 6, 6, 6, 4, 5,\n",
       "       5, 5, 6, 6, 0, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 0, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 4, 6, 4, 5,\n",
       "       6, 5, 6, 6, 0, 0, 1, 1, 6, 6, 6, 6, 6, 4, 4, 6, 5, 6, 0, 0, 1, 1,\n",
       "       6, 1, 6, 6, 6, 6, 6, 6, 6, 4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 4, 5, 5, 6, 0, 1, 1, 6, 6, 6, 6, 6, 6, 4, 5, 5, 6, 6, 0, 0,\n",
       "       0, 0, 1, 6, 6, 6, 6, 6, 4, 4, 5, 5, 5, 5, 6, 6, 0, 0, 0, 0, 1, 1,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 6, 3, 5, 5, 5, 5, 5, 5, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[3][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, ds, label_vocab):\n",
    "    pred_list = []\n",
    "    len_list = []\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        logits = model(input_ids, seg_ids)\n",
    "        pred = paddle.argmax(logits, axis=-1)\n",
    "        pred_list.append(pred.numpy())\n",
    "        len_list.append(lens.numpy())\n",
    "    preds = parse_decodes(ds, pred_list, len_list, label_vocab)\n",
    "    return preds\n",
    "\n",
    "def parse_decodes(ds, decodes, lens, label_vocab):\n",
    "    decodes = [x for batch in decodes for x in batch]  # [[]]\n",
    "    lens = [x for batch in lens for x in batch]        # []\n",
    "    id_label = dict(zip(label_vocab.values(), label_vocab.keys()))\n",
    "\n",
    "    outputs = []\n",
    "    for idx, end in enumerate(lens):\n",
    "        sent = ds.data[idx]['tokens'][:end]\n",
    "        tags = [id_label[x] for x in decodes[idx][1:end]]\n",
    "        sent_out = []\n",
    "        tags_out = []\n",
    "        words = \"\"\n",
    "        for s, t in zip(sent, tags):\n",
    "            if t.startswith('B-') or t == 'O':\n",
    "                if len(words):\n",
    "                    sent_out.append(words)\n",
    "                if t.startswith('B-'):\n",
    "                    tags_out.append(t.split('-')[1])\n",
    "                else:\n",
    "                    tags_out.append(t)\n",
    "                words = s\n",
    "            else:\n",
    "                words += s\n",
    "        if len(sent_out) < len(tags_out):\n",
    "            sent_out.append(words)\n",
    "        outputs.append(''.join(\n",
    "            [str((s, t)) for s, t in zip(sent_out, tags_out)]))\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = os.path.join('./data', 'checkpoints')\n",
    "\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# save_param_path = os.path.join(save_dir, 'bets_model_state.pdparams')\n",
    "\n",
    "# state_dict = paddle.load(save_param_path)\n",
    "# model.set_dict(state_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: msra_ner_results.txt, some examples are shown below: \n",
      "('中共中央', 'ORG')('致', 'O')('中国致公党十一大', 'ORG')('的', 'O')('贺', 'O')('词', 'O')('各', 'O')('位', 'O')('代', 'O')('表', 'O')('、', 'O')('各', 'O')('位', 'O')('同', 'O')('志', 'O')('：', 'O')('在', 'O')('中国致公党第十一次全国代表大会', 'ORG')('隆', 'O')('重', 'O')('召', 'O')('开', 'O')('之', 'O')('际', 'O')('，', 'O')('中国共产党中央委员会', 'ORG')('谨', 'O')('向', 'O')('大', 'O')('会', 'O')('表', 'O')('示', 'O')('热', 'O')('烈', 'O')('的', 'O')('祝', 'O')('贺', 'O')('，', 'O')('向', 'O')('致公党', 'ORG')('的', 'O')('同', 'O')('志', 'O')('们', 'O')('致', 'O')('以', 'O')('亲', 'O')('切', 'O')('的', 'O')('问', 'O')('候', 'O')('！', 'O')\n",
      "('在', 'O')('过', 'O')('去', 'O')('的', 'O')('五', 'O')('年', 'O')('中', 'O')('，', 'O')('致公党', 'ORG')('在', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('遵', 'O')('循', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('初', 'O')('级', 'O')('阶', 'O')('段', 'O')('的', 'O')('基', 'O')('本', 'O')('路', 'O')('线', 'O')('，', 'O')('努', 'O')('力', 'O')('实', 'O')('践', 'O')('致公党十大', 'ORG')('提', 'O')('出', 'O')('的', 'O')('发', 'O')('挥', 'O')('参', 'O')('政', 'O')('党', 'O')('职', 'O')('能', 'O')('、', 'O')('加', 'O')('强', 'O')('自', 'O')('身', 'O')('建', 'O')('设', 'O')('的', 'O')('基', 'O')('本', 'O')('任', 'O')('务', 'O')('。', 'O')\n",
      "('高', 'O')('举', 'O')('爱', 'O')('国', 'O')('主', 'O')('义', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('两', 'O')('面', 'O')('旗', 'O')('帜', 'O')('，', 'O')('团', 'O')('结', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('以', 'O')('及', 'O')('所', 'O')('联', 'O')('系', 'O')('的', 'O')('归', 'O')('侨', 'O')('、', 'O')('侨', 'O')('眷', 'O')('，', 'O')('发', 'O')('扬', 'O')('爱', 'O')('国', 'O')('革', 'O')('命', 'O')('的', 'O')('光', 'O')('荣', 'O')('传', 'O')('统', 'O')('，', 'O')('为', 'O')('统', 'O')('一', 'O')('祖', 'O')('国', 'O')('、', 'O')('振', 'O')('兴', 'O')('中华', 'LOC')('而', 'O')('努', 'O')('力', 'O')('奋', 'O')('斗', 'O')('；', 'O')('紧', 'O')('紧', 'O')('围', 'O')('绕', 'O')('国', 'O')('家', 'O')('的', 'O')('中', 'O')('心', 'O')('工', 'O')('作', 'O')('，', 'O')('联', 'O')('系', 'O')('改', 'O')('革', 'O')('和', 'O')('建', 'O')('设', 'O')('中', 'O')('的', 'O')('重', 'O')('大', 'O')('问', 'O')('题', 'O')('以', 'O')('及', 'O')('人', 'O')('民', 'O')('群', 'O')('众', 'O')('普', 'O')('遍', 'O')('关', 'O')('心', 'O')('的', 'O')('社', 'O')('会', 'O')('问', 'O')('题', 'O')('，', 'O')('深', 'O')('入', 'O')('开', 'O')('展', 'O')('调', 'O')('查', 'O')('研', 'O')('究', 'O')('，', 'O')('就', 'O')('经', 'O')('济', 'O')('建', 'O')('设', 'O')('、', 'O')('侨', 'O')('务', 'O')('政', 'O')('策', 'O')('、', 'O')('文', 'O')('教', 'O')('卫', 'O')('生', 'O')('、', 'O')('对', 'O')('外', 'O')('开', 'O')('放', 'O')('、', 'O')('精', 'O')('神', 'O')('文', 'O')('明', 'O')('建', 'O')('设', 'O')('等', 'O')('问', 'O')('题', 'O')('，', 'O')('提', 'O')('出', 'O')('了', 'O')('许', 'O')('多', 'O')('宝', 'O')('贵', 'O')('的', 'O')('意', 'O')('见', 'O')('和', 'O')('建', 'O')('议', 'O')('，', 'O')('受', 'O')('到', 'O')('有', 'O')('关', 'O')('方', 'O')('面', 'O')('高', 'O')('度', 'O')('重', 'O')('视', 'O')('；', 'O')('致公党中央', 'O')('领', 'O')('导', 'O')('人', 'O')('多', 'O')('次', 'O')('参', 'O')('加', 'O')('中共中央', 'ORG')('和', 'O')('国务院', 'ORG')('举', 'O')('行', 'O')('的', 'O')('民', 'O')('主', 'O')('党', 'O')('派', 'O')('人', 'O')('士', 'O')('座', 'O')('谈', 'O')('会', 'O')('、', 'O')('协', 'O')('商', 'O')('会', 'O')('，', 'O')('参', 'O')('与', 'O')('国', 'O')('家', 'O')('大', 'O')('政', 'O')('方', 'O')('针', 'O')('的', 'O')('协', 'O')('商', 'O')('，', 'O')('认', 'O')('真', 'O')('履', 'O')('行', 'O')('参', 'O')('政', 'O')('议', 'O')('政', 'O')('、', 'O')('民', 'O')('主', 'O')('监', 'O')('督', 'O')('职', 'O')('能', 'O')('；', 'O')('广', 'O')('大', 'O')('成', 'O')('员', 'O')('在', 'O')('做', 'O')('好', 'O')('本', 'O')('职', 'O')('工', 'O')('作', 'O')('的', 'O')('同', 'O')('时', 'O')('，', 'O')('把', 'O')('科', 'O')('技', 'O')('扶', 'O')('贫', 'O')('、', 'O')('智', 'O')('力', 'O')('支', 'O')('边', 'O')('作', 'O')('为', 'O')('为', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('建', 'O')('设', 'O')('服', 'O')('务', 'O')('的', 'O')('一', 'O')('项', 'O')('重', 'O')('要', 'O')('工', 'O')('作', 'O')('，', 'O')('不', 'O')('断', 'O')('开', 'O')('拓', 'O')('进', 'O')('取', 'O')('，', 'O')('取', 'O')('得', 'O')('了', 'O')('可', 'O')('喜', 'O')('的', 'O')('成', 'O')('绩', 'O')('，', 'O')('为', 'O')('促', 'O')('进', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('物', 'O')('质', 'O')('文', 'O')('明', 'O')('和', 'O')('精', 'O')('神', 'O')('文', 'O')('明', 'O')('建', 'O')('设', 'O')('作', 'O')('出', 'O')('了', 'O')('积', 'O')('极', 'O')('贡', 'O')('献', 'O')('；', 'O')('结', 'O')('合', 'O')('自', 'O')('身', 'O')('的', 'O')('特', 'O')('点', 'O')('，', 'O')('充', 'O')('分', 'O')('发', 'O')('挥', 'O')('与', 'O')('海', 'O')('外', 'O')('联', 'O')('系', 'O')('广', 'O')('泛', 'O')('的', 'O')('优', 'O')('势', 'O')('，', 'O')('积', 'O')('极', 'O')('开', 'O')('展', 'O')('海', 'O')('外', 'O')('联', 'O')('络', 'O')('工', 'O')('作', 'O')('，', 'O')('为', 'O')('促', 'O')('进', 'O')('祖', 'O')('国', 'O')('的', 'O')('和', 'O')('平', 'O')('统', 'O')('一', 'O')('作', 'O')('出', 'O')('了', 'O')('不', 'O')('懈', 'O')('的', 'O')('努', 'O')('力', 'O')('。', 'O')\n",
      "('在', 'O')('此', 'O')('，', 'O')('中共中央', 'ORG')('谨', 'O')('向', 'O')('致公党中央', 'ORG')('以', 'O')('及', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('致', 'O')('以', 'O')('崇', 'O')('高', 'O')('的', 'O')('敬', 'O')('意', 'O')('！', 'O')\n",
      "('不', 'O')('久', 'O')('前', 'O')('，', 'O')('中国共产党', 'ORG')('召', 'O')('开', 'O')('了', 'O')('举', 'O')('世', 'O')('瞩', 'O')('目', 'O')('的', 'O')('第十五次全国代表大', 'ORG')('会', 'O')('。', 'O')\n",
      "('这', 'O')('次', 'O')('代', 'O')('表', 'O')('大', 'O')('会', 'O')('是', 'O')('在', 'O')('中国', 'LOC')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('发', 'O')('展', 'O')('的', 'O')('关', 'O')('键', 'O')('时', 'O')('刻', 'O')('召', 'O')('开', 'O')('的', 'O')('历', 'O')('史', 'O')('性', 'O')('会', 'O')('议', 'O')('。', 'O')\n",
      "('大', 'O')('会', 'O')('高', 'O')('举', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('回', 'O')('顾', 'O')('一', 'O')('个', 'O')('世', 'O')('纪', 'O')('以', 'O')('来', 'O')('中国', 'LOC')('人', 'O')('民', 'O')('的', 'O')('奋', 'O')('斗', 'O')('历', 'O')('史', 'O')('，', 'O')('展', 'O')('望', 'O')('下', 'O')('个', 'O')('世', 'O')('纪', 'O')('５', 'O')('０', 'O')('年', 'O')('的', 'O')('发', 'O')('展', 'O')('前', 'O')('景', 'O')('，', 'O')('认', 'O')('真', 'O')('总', 'O')('结', 'O')('了', 'O')('中共十', 'ORG')('一', 'O')('届', 'O')('三', 'O')('中', 'O')('全', 'O')('会', 'O')('以', 'O')('来', 'O')('特', 'O')('别', 'O')('是', 'O')('十四大', 'ORG')('以', 'O')('来', 'O')('的', 'O')('实', 'O')('践', 'O')('经', 'O')('验', 'O')('，', 'O')('对', 'O')('中国', 'LOC')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('跨', 'O')('世', 'O')('纪', 'O')('的', 'O')('发', 'O')('展', 'O')('作', 'O')('出', 'O')('了', 'O')('全', 'O')('面', 'O')('部', 'O')('署', 'O')('。', 'O')\n",
      "('这', 'O')('次', 'O')('大', 'O')('会', 'O')('对', 'O')('于', 'O')('动', 'O')('员', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('，', 'O')('解', 'O')('放', 'O')('思', 'O')('想', 'O')('，', 'O')('实', 'O')('事', 'O')('求', 'O')('是', 'O')('，', 'O')('抓', 'O')('住', 'O')('有', 'O')('利', 'O')('时', 'O')('机', 'O')('，', 'O')('继', 'O')('续', 'O')('开', 'O')('拓', 'O')('前', 'O')('进', 'O')('，', 'O')('把', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('伟', 'O')('大', 'O')('事', 'O')('业', 'O')('全', 'O')('面', 'O')('推', 'O')('向', 'O')('２', 'O')('１', 'O')('世', 'O')('纪', 'O')('，', 'O')('具', 'O')('有', 'O')('极', 'O')('其', 'O')('重', 'O')('大', 'O')('和', 'O')('深', 'O')('远', 'O')('的', 'O')('意', 'O')('义', 'O')('。', 'O')\n",
      "('当', 'O')('前', 'O')('，', 'O')('在', 'O')('中共十五大', 'ORG')('精', 'O')('神', 'O')('的', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('在', 'O')('以', 'O')('江泽民', 'PER')('同', 'O')('志', 'O')('为', 'O')('核', 'O')('心', 'O')('的', 'O')('中共中央', 'ORG')('领', 'O')('导', 'O')('下', 'O')('，', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('正', 'O')('高', 'O')('举', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('同', 'O')('心', 'O')('同', 'O')('德', 'O')('，', 'O')('团', 'O')('结', 'O')('奋', 'O')('斗', 'O')('，', 'O')('沿', 'O')('着', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('的', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('道', 'O')('路', 'O')('阔', 'O')('步', 'O')('前', 'O')('进', 'O')('。', 'O')\n",
      "('实', 'O')('现', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('的', 'O')('宏', 'O')('伟', 'O')('目', 'O')('标', 'O')('，', 'O')('是', 'O')('中国共产党', 'ORG')('和', 'O')('作', 'O')('为', 'O')('参', 'O')('政', 'O')('党', 'O')('的', 'O')('各', 'O')('民', 'O')('主', 'O')('党', 'O')('派', 'O')('共', 'O')('同', 'O')('肩', 'O')('负', 'O')('的', 'O')('历', 'O')('史', 'O')('使', 'O')('命', 'O')('。', 'O')\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_data_loader, test_dataset, label_vocab)\n",
    "file_path = \"msra_ner_results.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write(\"\\n\".join(preds))\n",
    "# Print some examples\n",
    "print(\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\n",
    "    % file_path)\n",
    "print(\"\\n\".join(preds[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
